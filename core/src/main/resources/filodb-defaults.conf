{
  filodb {
    cassandra {
      hosts = ["localhost"]
      port = 9042
      keyspace = "filodb"
      admin-keyspace = "filodb_admin"
      # username = "abc"
      # password = "xyz"
      # read-timeout = 12 s
      # connect-timeout = 5 s
      # default-consistency-level = QUORUM.  NOTE: see this link for the string values:
      # http://docs.datastax.com/en/drivers/java/2.1/com/datastax/driver/core/ConsistencyLevel.html

      # CQL CREATE KEYSPACE options.  You will want to change these for production.
      keyspace-replication-options = "{'class': 'SimpleStrategy', 'replication_factor': '1'}"

      # NONE, LZ4, SNAPPY.  Compression of CQL traffic over the network.  Turn on for remote clusters,
      # can help by up to 20-30% - unless you use lz4-chunk-compress, in which case leave this off
      cql-compression = "NONE"

      # Compress columnar chunks using LZ4 and store them over CQL that way.
      # Use this instead of cql-compression for the best read performance
      lz4-chunk-compress = false

      # See http://docs.datastax.com/en/cql/3.1/cql/cql_reference/compressSubprop.html for valid values;
      # Use "" to turn off compression.  For the main chunks table only.
      sstable-compression = "LZ4Compressor"
    }

    spark {
      # The amount of time to wait for dataset creation, truncation, schema changes, etc.
      dataset-ops-timeout = 30s

      # The amount of time to wait for a dataset to finish flushing at the end of a DataFrame write
      flush-timeout = 5m

      # The port used by FiloDB coordinators on executor nodes and the driver to communicate with each
      # other via Akka clustering.  Leaving this commented out will default the port setting to 0, which
      # means find any free open port.
      # executor.port = 5444
      # driver.port = 5555
    }

    # Which column and metaStore to use, ie where to persist columnar chunks.
    # Valid values are "filodb.cassandra.CassandraStoreFactory", "in-memory"
    # Can also be the full class path to an implementation of StoreFactory
    store = "filodb.cassandra.CassandraStoreFactory"

    columnstore {
      # Number of cache entries for the table cache
      tablecache-size = 50

      # Maximum number of partitions that can be fetched at once that will still fit in
      # one Spark partition/thread when using IN clause in query.
      # If number of partitions are more than this limit then full table scan is performed.
      inquery-partitions-limit = 12
    }

    memtable {
      # Uncomment to enable mmap-file based memtable for persistence and easy recovery upon crashes.
      # Defaults to in-memory DB only which is lost upon restarts/crashes, but easier for testing.
      # local-filename = "/tmp/filodb.memtable"

      # Maximum rows per dataset/version before ingestRows throws back a PleaseWait.
      max-rows-per-table = 200000

      # Number of rows in memtable before flushes start being triggered
      flush-trigger-rows = 100000

      # The minimum amount of free memory required to accept new rows into memtable
      min-free-mb = 300

      # Chunk size for FiloMemTable - determines minimum # of rows before serializing to chunks
      filo.chunksize = 1000

      # Maximum delay before new rows in FiloMemTable are flushed to WAL and turned into chunks.
      # The bigger this is, the less WAL disk activity, but also the longer before ingestRows
      # returns acknowledgement.  Think of it as max delay before new ingested rows are guaranteed
      # to be recovered.
      write.interval = 1 s

      # This is to start flush on memtable when there is no write activity
      noactivity.flush.interval = 10 s
    }

    write-ahead-log {
      # Location to store memtable write ahead log files for each dataset
      memtable-wal-dir = /tmp/filodb/wal

      # Minimum size of the Memory mapped ByteBuffer allocated for write ahead log file in bytes
      mapped-byte-buffer-size = 128000

      # Change to true to reload existing WAL files during ingestion
      reload-wal-enabled = false

      # Change to true to create WAL files during ingestion
      write-ahead-log-enabled = false
    }

    metrics-logger {
      # Change to true to log internal metrics every tick (10s default, see Kamon.io docs)
      enabled = false

      # Filter different metrics to receive, can be a pattern, empty string to disable
      # ** = log everything / match all
      # trace = **
      # trace-segment = **
      # counter = **
      # gauge = **
      # histogram = **
      # min-max-counter = **
    }

    # Thread pool size (min and max) for filodb.core reprojection and I/O tasks.
    # NOTE: This definitely needs to be larger than the segment-batch-size, otherwise
    # all of the segments will be blocked waiting for reads.
    core-futures-pool-size = 128
    core-futures-max-pool-size = 128

    # Thread pool queue length.  When queue is full, tasks get run in calling thread.
    core-futures-queue-length = 1024

    reprojector {
      # Number of times to retry a segment write.  Exponential backoff included.
      retries = 3
      retry-base-timeunit = 5 s

      # Number of SegmentState entries to cache
      segment-cache-size = 1000

      # Number of segments to flush at a time during reprojection.  Serves to throttle ingestion;
      # higher number uses more memory and causes more write contention to the ColumnStore, but may be
      # slightly more efficient.  If tight on memory, or seeing write timeouts, reduce this number.
      # Ingesting small-segment datasets may require this to be raised.
      segment-batch-size = 16

      # Bulk Write Mode disables row replacement functionality, a potentially expensive part of the ingestion
      # pipeline.  It means new rows will just be appended as new chunks and existing rows with the same
      # primary key will not have skip indexes added for them.  It should only be enabled for bulk Spark
      # ingestion jobs that overwrite an entire table, so that failures will not cause issues.
      bulk-write-mode = false
    }

    hive {
      # Uncomment the below to enable automatic syncing of FiloDB datasets into Hive Metastore external
      # tables so that one does not need to register tables manually with the Hive store.
      # FiloDB tables in the cassandra keyspace below will be synced to the Hive database name below.
      # database-name = "filodb"
    }
  }

  akka {
    # Use SLF4J for deployed environment logging
    loggers = ["akka.event.slf4j.Slf4jLogger"]
    loglevel = "INFO"
    actor {
      provider = "akka.cluster.ClusterActorRefProvider"
      warn-about-java-serializer-usage = off
      debug {
        # To enable logging of every Akka message received by the various actors, uncomment the lines below,
        # then change the loglevel above to DEBUG
        # receive = on
        # autoreceive = on
        lifecycle = on
      }

      serializers {
        filoingest = "filodb.coordinator.IngestRowsSerializer"
      }

      serialization-bindings {
        "filodb.coordinator.IngestionCommands$IngestRows" = filoingest
      }
    }

    remote {
      log-remote-lifecycle-events = off
      netty.tcp {
        # Leave out the hostname, it will be automatically determined.
        # The Akka port will be overridden by filodb.spark.* settings
        port = 0
        send-buffer-size = 512000b
        receive-buffer-size = 512000b
        maximum-frame-size = 10 MiB
      }
    }

    cluster {
     # If a join request fails it will be retried after this period. Disable join retry by specifying "off".
      retry-unsuccessful-join-after = 10s

      auto-down-unreachable-after = 30s
      metrics.enabled = off
      failure-detector {
        heartbeat-interval = 5s
        acceptable-heartbeat-pause = 15s
        threshold = 12.0
        expected-response-after = 5s
      }
    }
  }
}
