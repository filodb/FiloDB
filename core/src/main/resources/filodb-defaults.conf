filodb {

  tasks {
    # Frequency with which new shard maps are published
    shardmap-publish-frequency = 5s

    # Internal task configs for handling lifecycle management and events
    timeouts {
      default = 8000ms
      initialization = 60s
      graceful-stop = 8000ms
      resolve-actor = 10s

      # The timeout before retrying status updates
      status-ack-timeout = 15s
    }
  }

  # # of shards each application/metric is spread out to = 2^spread
  default-spread = 1

  query {
    # Timeout for query engine subtree/ExecPlans for requests to sub nodes
    ask-timeout = 10 seconds

    stale-sample-after = 5 minutes

    # Maximum number of RangeVectors returned from any sub-plan QueryResult
    # vectors-limit = 150

    # Maximum number of samples to return per RangeVector/result time series
    sample-limit = 200
  }

  shard-manager {
    # Minimum time required between successive automatic shard reassignments done by ShardManager
    reassignment-min-interval = 2 hours
  }

  cassandra {
    hosts = ["localhost"]
    port = 9042
    keyspace = "filodb"
    admin-keyspace = "filodb_admin"
    # username = "abc"
    # password = "xyz"
    # read-timeout = 12 s
    # connect-timeout = 5 s
    # default-consistency-level = QUORUM.  NOTE: see this link for the string values:
    # http://docs.datastax.com/en/drivers/java/2.1/com/datastax/driver/core/ConsistencyLevel.html

    # Number of parallel chunkset writes at a time
    write-parallelism = 4

    # CQL CREATE KEYSPACE options.  You will want to change these for production.
    keyspace-replication-options = "{'class': 'SimpleStrategy', 'replication_factor': '1'}"

    # NONE, LZ4, SNAPPY.  Compression of CQL traffic over the network.  Turn on for remote clusters,
    # can help by up to 20-30% - unless you use lz4-chunk-compress, in which case leave this off
    cql-compression = "NONE"

    # Compress columnar chunks using LZ4 and store them over CQL that way.
    # Use this instead of cql-compression for the best read performance
    lz4-chunk-compress = false

    # See http://docs.datastax.com/en/cql/3.1/cql/cql_reference/compressSubprop.html for valid values;
    # Use "" to turn off compression.  For the main chunks table only.
    sstable-compression = "LZ4Compressor"

    # retry duration (including jitter) is configured to be little more than chunk-duration / groups-per-shard
    max-retry-attempts = 5
    retry-interval = 10s
    retry-interval-max-jitter = 10s

    ingestion-consistency-level = "ONE"
  }

  spark {
    # The amount of time to wait for dataset creation, truncation, schema changes, etc.
    dataset-ops-timeout = 30s

    # The amount of time to wait for a dataset to finish flushing at the end of a DataFrame write
    flush-timeout = 5m

    # The port used by FiloDB coordinators on executor nodes and the driver to communicate with each
    # other via Akka clustering.  Leaving this commented out will default the port setting to 0, which
    # means find any free open port.
    # executor.port = 5444
    # driver.port = 5555
  }

  # Which MemStore, ChunkSink (for chunk persistence) and MetaStore to use
  # Should be the full class path / FQCN to an implementation of StoreFactory
  store-factory = "filodb.coordinator.TimeSeriesNullStoreFactory"

  columnstore {
    # Number of cache entries for the table cache
    tablecache-size = 50

    # Maximum number of partitions that can be fetched at once that will still fit in
    # one Spark partition/thread when using IN clause in query.
    # If number of partitions are more than this limit then full table scan is performed.
    inquery-partitions-limit = 12
  }

  memstore {
    # Parallelism of persistance flush tasks. Should never be greater than groups-per-shard
    flush-task-parallelism = 2

    # Minimum amount of memory (as a memory string, eg 2MB, 1GB) to maintain in the write buffers.
    # This is also memory for other time series data structures.  Falling below this number means data will be
    # evicted from memory and possibly lost new data if not enough eviction happens.
    # It should be large enough to accommodate a new partition key, a new OffheapLFSortedIDMap structure
    # (a couple hundred bytes) plus 1000 write buffers.
    min-write-buffers-free = 5MB
  }

  # for standalone worker cluster configuration, see akka-bootstrapper

  # dataset-definitions:
  # See FiloServer.scala for a way to automatically define datasets at startup

  hive {
    # Uncomment the below to enable automatic syncing of FiloDB datasets into Hive Metastore external
    # tables so that one does not need to register tables manually with the Hive store.
    # FiloDB tables in the cassandra keyspace below will be synced to the Hive database name below.
    # database-name = "filodb"
  }
}

query-actor-mailbox {
  mailbox-type = "filodb.coordinator.QueryActorMailbox"
}

# Configuration for the open-source ingestion gateway
gateway {
  # TCP Port for Influx Line Protocol incoming data
  influx-port = 8007

  # Number of threads or parallel tasks serializing input records to container format for Kafka/sink
  producer-parallelism = 16

  # Minimum size of queue for each shard's records.  Must be a power of 2
  min-queue-size = 256

  # Maximum size of queue for each shard's records.  Must be a power of 2.  When the max queue size is reached
  # then the connection will stop accepting incoming records.
  max-queue-size = 16384

  # Amount of sleep the ingesting TCP server does when the queue for a given shard is full
  queue-full-wait = 100ms

  tcp {
    ssl-enabled = false
    netty-receive-buffer-size = 1048576
    netty-send-buffer-size = 1048576
  }
}

akka {

  test.single-expect-default = 10s

  extensions = ["filodb.coordinator.FilodbCluster",
                "com.romix.akka.serialization.kryo.KryoSerializationExtension$"]

  # Use SLF4J for deployed environment logging
  loggers = ["akka.event.slf4j.Slf4jLogger"]

  # will filter the log events using the backend configuration
  # (e.g. logback.xml) before they are published to the event bus.
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  loglevel = "INFO"
  actor {

    #https://doc.akka.io/docs/akka/2.5/project/migration-guide-2.4.x-2.5.x.html#additional-serialization-bindings
    # Set this to off to disable serialization-bindings defined in
    # additional-serialization-bindings. That should only be needed
    # for backwards compatibility reasons.
    enable-additional-serialization-bindings = off

    provider = "akka.cluster.ClusterActorRefProvider"
    warn-about-java-serializer-usage = off
    debug {
      # To enable logging of every Akka message received by the various actors, uncomment the lines below,
      # then change the loglevel above to DEBUG
      # receive = on
      # autoreceive = on
      lifecycle = on
    }

    # For details of kryo section see https://github.com/romix/akka-kryo-serialization
    kryo {
      # TODO: turn this off once finished debugging Kryo classes for serialization
      implicit-registration-logging = "true"

      kryo-custom-serializer-init = "filodb.coordinator.client.KryoInit"

      # Make the buffer size bigger as we send out chunks quite often
      buffer-size = 65536

      # automatic means fall back to FQCN's if class is not pre-registered.  This is safer.
      idstrategy = "automatic"

      mappings {
        # Set standard IDs here from say Scala up to 99.
        # DO NOT INCLUDE FiloDB classes here.  Those are better to put in Serializer.scala so they can be typechecked.
        "scala.Some" = 64
        "scala.Tuple2" = 65
        "scala.None$" = 66
        "scala.collection.immutable.Nil$" = 67
        "scala.collection.immutable.$colon$colon" = 68
        "scala.collection.mutable.ArrayBuffer" = 69
        "scala.collection.immutable.Vector" = 70
      }
    }

    serializers {
      kryo = "com.romix.akka.serialization.kryo.KryoSerializer"
    }

    serialization-bindings {
      "filodb.coordinator.client.IngestionCommands$IngestRows" = kryo
      "filodb.query.QueryCommand" = kryo
      "filodb.coordinator.client.QueryResponse" = kryo
      "filodb.core.query.Result" = kryo
      "filodb.coordinator.client.DatasetCommands$CreateDataset" = kryo
      "filodb.coordinator.StatusActor$EventEnvelope" = kryo
      "filodb.coordinator.StatusActor$StatusAck" = kryo
      "filodb.coordinator.CurrentShardSnapshot" = kryo

      "filodb.query.QueryResult" = kryo
      "filodb.query.QueryError" = kryo
      "filodb.query.exec.ExecPlan" = kryo
      "filodb.query.LogicalPlan" = kryo
    }

    # Just the defaults to start with. TODO optimize and pick the executor needed.
    shard-status-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
      # Configuration for the fork join pool
      fork-join-executor {
        # Min number of threads to cap factor-based parallelism number to
        parallelism-min = 2
        # Parallelism (threads) ... ceil(available processors * factor)
        parallelism-factor = 2.0
        # Max number of threads to cap factor-based parallelism number to
        parallelism-max = 10
      }
      # Throughput defines the maximum number of messages to be
      # processed per actor before the thread jumps to the next actor.
      # Set to 1 for as fair as possible.
      throughput = 100
    }
  }

  remote {
    log-remote-lifecycle-events = off
    netty {

      #https://doc.akka.io/docs/akka/2.5/project/migration-guide-2.4.x-2.5.x.html#mutual-tls-authentication-now-required-by-default-for-netty-based-ssl-transport
      ssl.require-mutual-authentication = off

      tcp {
        # Leave out the hostname, it will be automatically determined.
        # The Akka port will be overridden by filodb.spark.* settings
        port = 0
        send-buffer-size = 512000b
        receive-buffer-size = 512000b
        maximum-frame-size = 10 MiB
      }
    }
  }

  cluster {
    roles = [worker]

    # If a join request fails it will be retried after this period. Disable join retry by specifying "off".
    retry-unsuccessful-join-after = 10s

    # Auto downing is turned off by default.  See
    # http://doc.akka.io/docs/akka/2.3.16/scala/cluster-usage.html#Automatic_vs__Manual_Downing
    # Instead, we use this repo to provide smarter downing solutions:
    # https://github.com/TanUkkii007/akka-cluster-custom-downing
    # Note that this strategy should work well but for fixed size clusters QuorumLeader might be better
    downing-provider-class = "tanukki.akka.cluster.autodown.MajorityLeaderAutoDowning"

    metrics.enabled = off
    failure-detector {
      heartbeat-interval = 5s
      acceptable-heartbeat-pause = 15s
      threshold = 12.0
      expected-response-after = 5s
    }
  }

  # Just the defaults to start with. TODO optimize and pick the executor needed.
  shard-status-dispatcher {
    # Dispatcher is the name of the event-based dispatcher
    type = Dispatcher
    # What kind of ExecutionService to use
    executor = "fork-join-executor"
    # Configuration for the fork join pool
    fork-join-executor {
      # Min number of threads to cap factor-based parallelism number to
      parallelism-min = 2
      # Parallelism (threads) ... ceil(available processors * factor)
      parallelism-factor = 2.0
      # Max number of threads to cap factor-based parallelism number to
      parallelism-max = 10
    }
    # Throughput defines the maximum number of messages to be
    # processed per actor before the thread jumps to the next actor.
    # Set to 1 for as fair as possible.
    throughput = 100
  }

  # Be sure to terminate/exit JVM process after Akka shuts down.  This is important for the
  # custom downing provider's split brain resolution to work properly.  Basically, the minority
  # group will shut down itself and exit the process, helping to bring newer nodes online.
  coordinated-shutdown.exit-jvm = on
}


custom-downing {
  stable-after = 20s

  majority-leader-auto-downing {
    majority-member-role = "worker"    # Must match akka.cluster.roles
    down-if-in-minority = true
    shutdown-actor-system-on-resolution = true
  }
}