filodb {
  v2-cluster-enabled = false
  cluster-discovery {
    // set this to a smaller value (like 30s) at thee query entry points
    // if FiloDB HTTP API is indeed the query entry point, this should be overridden to small value
    // so that failure detection is quick.
    failure-detection-interval = 15 minutes
    num-nodes = 2

    # one of the two below properties should be enabled
    # enable this to use the k8s stateful sets mode and its hostname to extract ordinal
    # k8s-stateful-sets-hostname-format = "filodb-cluster-{}.mydomain.com"
    #
    # use explicit hostlist - enabled during development only
    # host-list = [
    #    "127.0.0.1:2552",
    #    "127.0.0.1:3552"
    # ]
    #
    # indicates the ordinal for localhost - enabled during development only
    # localhost-ordinal = 1
  }

  # list of paths to dataset + ingestion config files
  # deprecated in favor of inline-dataset-configs
  dataset-configs = [ ]

  # As an alternative to dataset-configs, one can inline list each dataset/stream ingestion config
  # here, which might be more convenient in some cases than listing file paths (eg tests, reuse of shared configs).
  # This is only used if dataset-configs is empty.
  # inline-dataset-configs = [
  #   {
  #     dataset = "prometheus"
  #     definition { ... }
  #     sourceconfig { ... }
  #   },
  # ]
  inline-dataset-configs = []

  # Definition of cluster-wide partition key scheme.  The partition key defines each unique time series,
  # such as labels or tags, and is used for sharding time series across the cluster.
  # The below definition is standard for Prometheus schema.
  # Please do not modify unless you know what you are doing; some internal code esp in gateway module is sensitive
  # to the exact definitions.
  partition-schema {
    # Typical column types used: map, string.  Also possible: ts,long,double
    # NOTE: the metric column needs underscores to escape conflicts with tag labels named the same.
    columns = ["_metric_:string", "tags:map"]

    # Predefined keys allow space to be saved for over the wire tags with the given keys
    # WARN: 1. It is suggested to only ADD to predefined keys that are not used already,
    #          which means only new tags with underscore prefixed and suffixed.
    #       2. Changing/Renaming existing keys will cause incorrect tags to be reported and probably require a clean wipe.
    #       3. Even when adding new keys, be careful. If that tag is already being used, there is no
    #          equality of partKeys between old and new time series.
    predefined-keys = ["_ws_", "_ns_", "app", "__name__", "instance", "dc", "le", "job", "exporter", "_pi_"]
    # FYI: _pi_ stands for publish interval; _ws_ stands for workspace, _ns_ stands for namespace

    options {
      # Copy tags can be used to create a new labelPair from one of the existing labelPair in a TimeSeries.
      # It honors the order in which the label keys are represented against a new key.
      # Eg: For copyTags `"_ns_" = [ "exporter", "job" ]`, when an incoming TimeSeries already has both `exporter`
      #     and `job`, then it will pickup only the value from `exporter` for `_ns_`.
      copyTags = {
        "_ns_" = ["_ns", "exporter", "job"]
      }
      ignoreShardKeyColumnSuffixes = {"_metric_" = ["_bucket", "_count", "_sum"]}
      ignoreTagsOnPartitionKeyHash = ["le"]
      metricColumn = "_metric_"
      # shard key columns should be in hierarchical order
      shardKeyColumns = ["_ws_", "_ns_", "_metric_"]

      # Multi-column facets to be created on "partition-schema" columns.
      # Eg: below config creates a facet "all-hosts-availZone" by concatenating
      # the column values ("avail_zone", "rack", "host") "\u03C0" as delimiter
      # If the data is hierarchical in nature, This feature lets us fetch the metadata very efficiently by using
      # lucene facet feature.
      #
      # Note: only "string" column type supported.
      # multiColumnFacets = {
      #       all-hosts-availZone = ["avail_zone", "rack", "host"]
      # }
      multiColumnFacets = {}
    }
  }

  # Definitions of possible data schemas to be used in all datasets
  # Each one must have a unique name and column schema.
  # FiloDB will refuse to start if the schema definitions have errors.  Use the validateSchemas CLI command to check.
  # Please do not modify unless you know what you are doing; some internal code esp in gateway module is sensitive
  # to the exact definitions.
  schemas {
    gauge {
      # Each column def is of name:type format.  Type may be ts,long,double,string,int
      # The first column must be ts or long
      columns = ["timestamp:ts", "value:double:detectDrops=false"]

      # Default column to query using PromQL
      value-column = "value"

      # Downsampling configuration.  See doc/downsampling.md
      downsamplers = [ "tTime(0)", "dMin(1)", "dMax(1)", "dSum(1)", "dCount(1)", "dAvg(1)" ]

      # The marker implemention that determines row numbers at which to downsample
      downsample-period-marker = "time(0)"

      # If downsamplers are defined, then the downsample schema must also be defined
      downsample-schema = "ds-gauge"
    }

    # For metrics from Prom or other places without a known type, we treat them as gauges
    # The column name is different to be sure of no hash conflicts
    untyped {
      columns = ["timestamp:ts", "number:double"]
      value-column = "number"
      downsamplers = []
    }

    prom-counter {
      columns = ["timestamp:ts", "count:double:detectDrops=true"]
      value-column = "count"
      downsamplers = ["tTime(0)", "dLast(1)"]
      # Downsample periods are determined by counter dips of the counter column
      downsample-period-marker = "counter(1)"
      downsample-schema = "prom-counter"
    }

    delta-counter {
      columns = ["timestamp:ts", "count:double:{detectDrops=false,delta=true}"]
      value-column = "count"
      downsamplers = ["tTime(0)", "dSum(1)"]
      downsample-period-marker = "time(0)"
      downsample-schema = "delta-counter"
    }

    prom-histogram {
      columns = ["timestamp:ts",
                 "sum:double:detectDrops=true",
                 "count:double:detectDrops=true",
                 "h:hist:counter=true"]
      value-column = "h"
      downsamplers = ["tTime(0)", "dLast(1)", "dLast(2)", "hLast(3)"]
      # Downsample periods are determined by counter dips of the count column
      downsample-period-marker = "counter(2)"
      downsample-schema = "prom-histogram"
    }

    delta-histogram {
      columns = ["timestamp:ts",
        "sum:double:{detectDrops=false,delta=true}",
        "count:double:{detectDrops=false,delta=true}",
        "h:hist:{counter=false,delta=true}"]
      value-column = "h"
      downsamplers = ["tTime(0)", "dSum(1)", "dSum(2)", "hSum(3)"]
      downsample-period-marker = "time(0)"
      downsample-schema = "delta-histogram"
    }

    # PreAgg Schema defintions.
    # preagg-gauge and preagg-delta-counter have same schemas. Defining two different data-schemas to enable
    # additional features/functionalities at query-time. preaggregated delta-histogram data has same schema
    # as raw delta-histogram data, so no need to define additional schema for preaggregated data.
    preagg-gauge {
      columns = [
        "timestamp:ts",
        "count:double:detectDrops=false",
        "min:double:detectDrops=false",
        "sum:double:detectDrops=false",
        "max:double:detectDrops=false",
      ]
      value-column = "sum"
      downsamplers = [] // downsampling is disabled by default
    }
    preagg-delta-counter {
      columns = [
        "timestamp:ts",
        "count:double:{detectDrops=false,delta=true}",
        "min:double:detectDrops=false",
        "sum:double:{detectDrops=false,delta=true}",
        "max:double:detectDrops=false",
      ]
      value-column = "sum"
      downsamplers = [] // downsampling is disabled by default
    }

    # Used for downsampled gauge data
    ds-gauge {
      columns = [ "timestamp:ts", "min:double", "max:double", "sum:double", "count:double", "avg:double" ]
      value-column = "avg"
      downsamplers = []
    }
  }

  quotas {
    # if one is not defined for data source, this number is used for all limits
    default = 2000000000
    # prometheus {
    #   defaults = [100, 500, 10000, 100000]
    #   custom = [
    #     {
    #       shardKeyPrefix = ["myWs", "myNs", "myMetricName"]
    #       quota = 10000
    #     }
    #   ]
    # }
  }

  tasks {
    # Frequency with which new shard maps are published
    shardmap-publish-frequency = 5s

    # Internal task configs for handling lifecycle management and events
    timeouts {
      default = 8000ms
      initialization = 60s
      graceful-stop = 8000ms
      resolve-actor = 10s

      # The timeout before retrying status updates
      status-ack-timeout = 15s
    }
  }

  scheduler {
    enable-assertions = false
  }

  profiler {
    enable-simple-profiler = true
    sample-rate = 100ms
    report-interval = 180s
    top-count = 50
  }

  # # of shards each application/metric is spread out to = 2^spread
  spread-default = 1
  # default spread can be overriden for a specific sharding key combination.
  # Eg: If "__name__, _ns" are your sharding key, for a _ns "App-Test001" the spread can be overriden as follows:
  # spread-assignment = [ { _ws_ = demo, _ns_ = App-Test001, _spread_ = 5 } ]
  spread-assignment = []

  shard-key-level-ingestion-metrics-enabled = true
  # Time between each TenantIngestionMetering query/publish job
  metering-query-interval = 15 minutes
  # info config used for metric breakdown
  cluster-type = "raw" // possible values: downsample, raw, recRule, aggregates etc.
  # Name of the deployment partition that this FiloDB cluster belongs to
  deployment-partition-name = "local"

  query {
    translate-prom-to-filodb-histogram = true
    # Timeout for query engine subtree/ExecPlans for requests to sub nodes
    # Higher default until we have a way to really timeout query execution at leaves
    ask-timeout = 120 seconds

    stale-sample-after = 5 minutes

    # Maximum number of samples to return in a query
    sample-limit = 1000000

    # Binary Join Cardinality limit
    join-cardinality-limit = 25000

    # Group by Cardinality limit
    group-by-cardinality-limit = 1000

    # Maximum result size returnable from any ExecPlan::execute call (including nested calls)
    # If this is updated, akka.remote.netty.tcp.maximum-frame-size should be updated, too.
    result-byte-limit = 18 MiB

    # If true, an exception is thrown if any ExecPlan::execute result size is greater than result-byte-limit.
    enforce-result-byte-limit = false

    # Minimum step required for a query
    min-step = 5 seconds

    # Parallelism (query threadpool per dataset) ... ceil(available processors * factor)
    threads-factor = 1.0

    # Maximum number of steps/windows to use the RangeVectorAggregator.fastReduce aggregators.  This aggregator
    # uses memory proportional to the # of windows, rather than the # of time series aggregated; it can speed up
    # high cardinality aggregations in particular.
    fastreduce-max-windows = 50

    # Enable faster rate/increase/delta calculations. Depends on drop detection in chunks (detectDrops=true)
    faster-rate = true

    # Choices are "legacy", "antlr", and "shadow". Shadow mode uses legacy but also checks antlr for errors.
    parser = "antlr"

    routing {
      # not currently used
    }

    # Config values are used when partialResults query parameter is not provided
    allow-partial-results-metadataquery = true
    allow-partial-results-rangequery = false

    circuit-breaker {
      # query circuit breaker enabled or not
      enabled = false
      # open the circuit when number of failures reaches this count
      open-when-num-failures = 5
      # move to half-open after this time
      reset-timeout = 15 seconds
      # backoff-factor when trying to half-open
      exp-backoff-factor = 1.5
      # maximum time in open state
      max-reset-timeout = 5 minutes
    }
  }

  shard-manager {
    # Minimum time required between successive automatic shard reassignments done by ShardManager
    reassignment-min-interval = 2 hours
    # Flag to enable
    enable-k8s-stateful-shard-strategy = false
    k8s-strategy {
        max-hostname-lookup-attempts = 8
    }
  }

  cassandra {
    hosts = ["localhost"]
    port = 9042
    keyspace = "filodb"
    downsample-keyspace = "filodb_downsample"
    admin-keyspace = "filodb_admin"
    # username = "abc"
    # password = "xyz"
    # read-timeout = 12 s
    # connect-timeout = 5 s
    # default-consistency-level = QUORUM.  NOTE: see this link for the string values:
    # http://docs.datastax.com/en/drivers/java/2.1/com/datastax/driver/core/ConsistencyLevel.html

    # Number of parallel chunkset writes at a time
    write-parallelism = 4

    # CQL CREATE KEYSPACE options.  You will want to change these for production.
    keyspace-replication-options = "{'class': 'SimpleStrategy', 'replication_factor': '1'}"

    # NONE, LZ4, SNAPPY.  Compression of CQL traffic over the network.  Turn on for remote clusters,
    # can help by up to 20-30% - unless you use lz4-chunk-compress, in which case leave this off
    cql-compression = "NONE"

    # Compress columnar chunks using LZ4 and store them over CQL that way.
    # Use this instead of cql-compression for the best read performance
    lz4-chunk-compress = false

    # See http://docs.datastax.com/en/cql/3.1/cql/cql_reference/compressSubprop.html for valid values;
    # Use "" to turn off compression.  For the main chunks table only.
    sstable-compression = "LZ4Compressor"

    # retry duration (including jitter) is configured to be little more than chunk-duration / groups-per-shard
    max-retry-attempts = 5
    retry-interval = 10s
    retry-interval-max-jitter = 10s

    # http://docs.datastax.com/en/drivers/java/2.1/com/datastax/driver/core/ConsistencyLevel.html
    checkpoint-read-consistency-level = "LOCAL_QUORUM"
    ingestion-consistency-level = "LOCAL_ONE"
    default-read-consistency-level = "LOCAL_ONE"

    # Number of splits in the partitionKeysByUpdateTime table to keep cassandra partition size under control
    #  1mil max pks per shard per hour / 200 splits
    #      = 5000 keys per split * 1000 bytes per pk
    #      = 5MB per cass partition. Goal is to keep it under 10MB.
    pk-by-updated-time-table-num-splits = 200

    # TTL for rows in ingestion time index tables. Ensure it is long enough to ensure that
    # downsampler or repair migration will complete in that time
    write-time-index-ttl = 3 days

    # Creation of tables is enabled. Do not set to true in production to avoid
    # multiple nodes trying to create table at once
    create-tables-enabled = false

    # amount of parallelism to introduce in the token scan queries. This controls number of spark partitions
    # increase if the number of splits seen in cassandra reads is low and spark jobs are slow, or
    # if we see Cassandra read timeouts in token range scans.
    num-token-range-splits-for-scans = 20

    # Parallel scans per shard. Use this property and num-token-range-splits-for-scans to
    # issue parallel queries to cassandra during index load to index reduce recovery times
    index-scan-parallelism-per-shard = 2

  }

  downsampler {

    chunk-downsampler-enabled = true

    # See filodb.downsampler.chunk.SparkSessionFactory for details.
    spark-session-factory = "filodb.downsampler.chunk.DefaultSparkSessionFactory"

    # Name of the dataset from which to downsample
    # raw-dataset-name = "prometheus"

    # Use to override cassandra Session Provider class name used by downsampler
    # cass-session-provider-fqcn = fqcn

    # Number of time series to operate on at one time. Reduce if there is much less memory available
    cass-write-batch-size = 250

    # Number of rows to read in one fetch. Reduce if we see Cassandra read timeouts
    cass-read-fetch-size = 5000

    # Amount of time to wait for a Cassandra write to finish before proceeding to next batch of partitions
    cassandra-write-timeout = 10.minutes

    # How much to increase userTime range by to formulate ingestionTime range to search for chunks to downsample.
    # Recommended to be at least twice chunkDuration of raw dataset to accommodate for early/late arriving data
    widen-ingestion-time-range-by = 2h

    # whether to sleep at end for metrics flush
    should-sleep-for-metrics-flush = true

    # configure only these three items.
    downsample-store-config {
      # Amount of ingestion time to query to create one chunk of downsampled data
      flush-interval = 6h

      # number of items in the downsampled chunk
      max-chunks-size = 400

      # Read parallelism in downsample cluster
      demand-paging-parallelism = 30

      # Limits maximum amount of data a single leaf query can scan per shard
      max-data-per-shard-query = 100 MB

      # Write buffer size, in bytes, for blob columns (histograms, UTF8Strings).  Since these are variable data types,
      # we need a maximum size, not a maximum number of items.
      max-blob-buffer-size = 15000

      # This block memory is used for overflow of write buffers and for storing encoded downsample chunks
      # Configure to NumPartitionsPerCassWrite * (EstChunkSize * EstNumChunksPerPartition) + buffer
      # 10000 * (1KB per col * 6 cols * 3 chunks) + buffer
      shard-mem-size = 400 MB

    }

    # If non-empty, partition will be downsampled only if it matches one of the allow filters
    allow-filters = [
      # {
      #  tagA1 = value1
      #  tagA2 = value2
      #},
      #{
      #  tagB1 = value1
      #  tagB2 = value2
      #}
    ]

    # Partition will be downsampled only if it does not match every allow filter.
    # If allow is non-empty, this list can be used to further filter out items in allow
    # If allow is empty, this list flags data not eligible foe downsampling
    block-filters = [
      #{
      #  tagB1 = value1
      #  tagB2 = value2
      #  tagB3 = value3
      #}
    ]

    # Downsampling of TS Partition will be traced if it matches the filter below
    trace-filters = [
      #{
      #  tagB1 = value1
      #  tagB2 = value2
      #  tagB3 = value3
      #}
    ]

    data-export {
      enabled = false

      # Spark SaveMode / options.
      save-mode = "error"
      format = "csv"
      options = {
        "header": "true"
      }

      # Describe the sequence of labels that compose a rule-group's key.
      # A time-series should match at most one key.
      key-labels = ["_ws_"]
      # These labels will be dropped from every exported row.
      drop-labels = ["_ws_", "drop-label1"]

      bucket = "file://<path-to-file>"

      # Each row's labels are compared against all rule-group keys. If a match is found,
      #   the row's labels are compared *sequentially* against each of the group's rules until
      #   a rule meets both of the following criteria:
      #       (1) No "block" filters are matched.
      #       (2) The "allow" filters are either matched or empty.
      # This rule will be applied to the row. This means that row data is/isn't exported
      #   depending on the matched filters, and any other rule effects are also applied (such as dropped labels).
      # If a matching rule (or group) are not found, the data will not be exported.
      groups = [
        {
          key = ["ws-foo"]
          rules = [
            {
              allow-filters = [
                ["_ns_=\"my_ns\"", "bar!=\"baz\""],
                ["_ns_=\"my_other_ns\""]
              ]
              block-filters = [
                ["_metric_=~\".*bad_suffix\""]
              ]
              drop-labels = ["label1", "other_label"]
            }
          ]
        }
      ]

      # Specifies how to generate a path to an exported time-series.
      # The sequence of key-value pairs describe a directory path, where the
      #   time-series are exported to the final directory.
      # For example, if the path-spec is:
      #     path-spec = [
      #       "key1", "value1",
      #       "key2", "value2"
      #     ]
      # then the exported time-series will be stored at the path:
      #     ~/key1=value1/key2=value2/<file>
      # All {{label-name}} strings will be replaced with the time-series label's value.
      # All <<time-spec>> strings will be replaced with the result of formatting
      #   the export window's end time with the time-spec string.
      path-spec = [
        "ws",   "{{_ws_}}-suffix1",
        "year", "<<YYYY>>-suffix2",
        "api",  "v1",
        "ns",   "{{_ns_}}-suffix3"
      ]
    }
  }

  ds-index-job {

    # Name of the dataset from which to downsample
    # raw-dataset-name = "prometheus"

    # Use to override cassandra Session Provider class name used by downsampler
    # cass-session-provider-fqcn = fqcn

    # Number of time series to operate on at one time. Reduce if there is much less memory available
    cass-write-batch-size = 10000

    # Maximum time to wait during cassandra reads to form a batch of partitions to downsample
    cass-write-batch-time = 3s

    # amount of parallelism to introduce in the spark job. This controls number of spark partitions
    # increase if the number of splits seen in cassandra reads is low and spark jobs are slow.
    splits-per-node = 1

    # Amount of time to wait for a Cassandra write to finish before proceeding to next batch of partitions
    cassandra-write-timeout = 1.minutes
  }

  spark {
    # The amount of time to wait for dataset creation, truncation, schema changes, etc.
    dataset-ops-timeout = 30s

    # The amount of time to wait for a dataset to finish flushing at the end of a DataFrame write
    flush-timeout = 5m

    # The port used by FiloDB coordinators on executor nodes and the driver to communicate with each
    # other via Akka clustering.  Leaving this commented out will default the port setting to 0, which
    # means find any free open port.
    # executor.port = 5444
    # driver.port = 5555
  }

  # Which MemStore, ChunkSink (for chunk persistence) and MetaStore to use
  # Should be the full class path / FQCN to an implementation of StoreFactory
  store-factory = "filodb.coordinator.TimeSeriesNullStoreFactory"

  columnstore {
    # Number of cache entries for the table cache
    tablecache-size = 50

    # Maximum number of partitions that can be fetched at once that will still fit in
    # one Spark partition/thread when using IN clause in query.
    # If number of partitions are more than this limit then full table scan is performed.
    inquery-partitions-limit = 12
  }

  memstore {
    # Parallelism of persistance flush tasks. Should never be greater than groups-per-shard
    flush-task-parallelism = 2

    # Minimum amount of memory as percentage of capacity to maintain in the block manager.
    # Increase if very large volume of ODP queries are expected within headroom task interval since
    # eviction will not happen during queries.
    ensure-block-memory-headroom-percent = 5

    # Headroom to maintain on the count of TSPs on the heap as a percent of
    # value in max-partitions-on-heap-per-shard config.
    # Increase if very high cardinality ODP queries are expected within headroom task interval since
    # eviction will not happen during queries.
    ensure-tsp-count-headroom-percent = 5

    # Minimum amount of memory as percentage of capacity to maintain in the native memory manager.
    # This is memory used for write-buffers, off-heap partKey and chunkMap. Falling below the headroom
    # threshold means data will be force-evicted from memory and new data is dropped if not enough eviction can happen.
    # Increase if very high cardinality ODP queries are expected within headroom task interval since
    # eviction will not happen during queries.
    ensure-native-memory-headroom-percent = 5

    # Maximum number of partitions to target on heap per shard. If numPartitions exceed this value
    # eviction will trigger on the shard. Set this by considering java heap setting, and total size
    # of partition keys and chunk-map on off-heap. Decreasing this will free off-heap memory more quickly.
    # Default to a high number - tune if needed
    max-partitions-on-heap-per-shard = 2000000

    # Number of bytes of offheap mem to allocate to write buffers for all shards.  Ex. 1000MB, 1G, 2GB
    # Note: this memory is shared across all configued datasets on a node.
    ingestion-buffer-mem-size = 200MB

    # At the cost of some extra heap memory, we can track queries holding shared lock for a long time
    # and starving the exclusive access of lock for eviction
    track-queries-holding-eviction-lock = true

    # Whether or not in TimeSeriesShard, faceting is enabled in lucene index for shard key labels.
    # If all of the index-faceting-enabled-* properties are false, faceting is fully disabled.
    # Disable if performance cost of faceting even the shard key label are too high
    index-faceting-enabled-shard-key-labels = true

    # Whether or not in TimeSeriesShard, faceting is enabled in lucene index for ALL labels.
    # If all of the index-faceting-enabled-* properties are false, faceting is fully disabled.
    # Disable if performance cost of faceting all labels is too high
    index-faceting-enabled-for-all-labels = true
  }

  # for standalone worker cluster configuration, see akka-bootstrapper

  # dataset-definitions:
  # See FiloServer.scala for a way to automatically define datasets at startup

  hive {
    # Uncomment the below to enable automatic syncing of FiloDB datasets into Hive Metastore external
    # tables so that one does not need to register tables manually with the Hive store.
    # FiloDB tables in the cassandra keyspace below will be synced to the Hive database name below.
    # database-name = "filodb"
  }
}

# Configuration for the open-source ingestion gateway
gateway {
  # TCP Port for Influx Line Protocol incoming data
  influx-port = 8007

  # Number of threads or parallel tasks serializing input records to container format for Kafka/sink
  producer-parallelism = 16

  # Minimum size of queue for each shard's records.  Must be a power of 2
  min-queue-size = 256

  # Maximum size of queue for each shard's records.  Must be a power of 2.  When the max queue size is reached
  # then the connection will stop accepting incoming records.
  max-queue-size = 262144

  # Amount of sleep the ingesting TCP server does when the queue for a given shard is full
  queue-full-wait = 100ms

  tcp {
    ssl-enabled = false
    netty-receive-buffer-size = 1048576
    netty-send-buffer-size = 1048576
  }
}

akka {

  http.server.request-timeout = 1m
  test.single-expect-default = 10s

  extensions = ["filodb.coordinator.FilodbCluster"]

  # Use SLF4J for deployed environment logging
  loggers = ["akka.event.slf4j.Slf4jLogger"]

  # will filter the log events using the backend configuration
  # (e.g. logback.xml) before they are published to the event bus.
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  loglevel = "INFO"
  actor {

    #https://doc.akka.io/docs/akka/2.5/project/migration-guide-2.4.x-2.5.x.html#additional-serialization-bindings
    # Set this to off to disable serialization-bindings defined in
    # additional-serialization-bindings. That should only be needed
    # for backwards compatibility reasons.
    enable-additional-serialization-bindings = off

    provider = "akka.cluster.ClusterActorRefProvider"
    warn-about-java-serializer-usage = off
    debug {
      # To enable logging of every Akka message received by the various actors, uncomment the lines below,
      # then change the loglevel above to DEBUG
      # receive = on
      # autoreceive = on
      lifecycle = on
    }

    serializers {
      kryo = "io.altoo.akka.serialization.kryo.KryoSerializer"
    }

    serialization-bindings {
      "filodb.coordinator.client.IngestionCommands$IngestRows" = kryo
      "filodb.query.QueryCommand" = kryo
      "filodb.coordinator.client.QueryResponse" = kryo
      "filodb.core.query.Result" = kryo
      "filodb.coordinator.client.DatasetCommands$CreateDataset" = kryo
      "filodb.coordinator.StatusActor$EventEnvelope" = kryo
      "filodb.coordinator.StatusActor$StatusAck" = kryo
      "filodb.coordinator.CurrentShardSnapshot" = kryo

      "filodb.query.StreamQueryError" = kryo
      "filodb.query.StreamQueryResult" = kryo
      "filodb.query.StreamQueryResultHeader" = kryo
      "filodb.query.StreamQueryResultFooter" = kryo
      "filodb.query.QueryResult" = kryo
      "filodb.query.QueryError" = kryo
      "filodb.query.exec.ExecPlan" = kryo
      "filodb.query.LogicalPlan" = kryo
    }

    # Reduce the number of threads used by default by the fork-join pool, as it's not really doing much work.
    default-dispatcher.fork-join-executor {
      parallelism-factor = 2.0
      parallelism-max = 32
    }

    # Just the defaults to start with. TODO optimize and pick the executor needed.
    shard-status-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
      # Configuration for the fork join pool
      fork-join-executor {
        # Min number of threads to cap factor-based parallelism number to
        parallelism-min = 2
        # Parallelism (threads) ... ceil(available processors * factor)
        parallelism-factor = 2.0
        # Max number of threads to cap factor-based parallelism number to
        parallelism-max = 10
      }
      # Throughput defines the maximum number of messages to be
      # processed per actor before the thread jumps to the next actor.
      # Set to 1 for as fair as possible.
      throughput = 100
    }
  }

  remote {
    log-remote-lifecycle-events = off
    netty {

      #https://doc.akka.io/docs/akka/2.5/project/migration-guide-2.4.x-2.5.x.html#mutual-tls-authentication-now-required-by-default-for-netty-based-ssl-transport
      ssl.require-mutual-authentication = off

      tcp {
        # Leave out the hostname, it will be automatically determined.
        # The Akka port will be overridden by filodb.spark.* settings
        port = 0
        send-buffer-size = 1024000b
        receive-buffer-size = 1024000b
        # If this is updated, filodb.query.result-byte-limit should be updated, too.
        maximum-frame-size = 18 MiB
      }
    }
  }

  cluster {
    roles = [worker]

    # If a join request fails it will be retried after this period. Disable join retry by specifying "off".
    retry-unsuccessful-join-after = 10s

    # Auto downing is turned off by default.  See
    # http://doc.akka.io/docs/akka/2.3.16/scala/cluster-usage.html#Automatic_vs__Manual_Downing
    # Instead, we use this repo to provide smarter downing solutions:
    # https://github.com/sisioh/akka-cluster-custom-downing
    # Note that this strategy should work well but for fixed size clusters QuorumLeader might be better
    downing-provider-class = "org.sisioh.akka.cluster.custom.downing.MajorityLeaderAutoDowning"

    metrics.enabled = off
    failure-detector {
      heartbeat-interval = 5s
      acceptable-heartbeat-pause = 15s
      threshold = 12.0
      expected-response-after = 5s
    }
  }

  # Be sure to terminate/exit JVM process after Akka shuts down.  This is important for the
  # custom downing provider's split brain resolution to work properly.  Basically, the minority
  # group will shut down itself and exit the process, helping to bring newer nodes online.
  coordinated-shutdown.exit-jvm = on
}

akka-kryo-serialization {
  # For details of kryo section see https://github.com/altoo-ag/akka-kryo-serialization
  implicit-registration-logging = "false"
  kryo-trace = "false"

  kryo-initializer = "filodb.coordinator.client.KryoInit"

  # Make the buffer size bigger as we send out chunks quite often
  buffer-size = 65536

  # compress after ser and decompress before deser
  post-serialization-transformations = "lz4"

  # automatic means fall back to FQCN's if class is not pre-registered.  This is safer.
  id-strategy = "automatic"

  mappings {
    # Set standard IDs here from say Scala up to 99.
    # DO NOT INCLUDE FiloDB classes here.  Those are better to put in Serializer.scala so they can be typechecked.
    "scala.Some" = 64
    "scala.Tuple2" = 65
    "scala.None$" = 66
    "scala.collection.immutable.Nil$" = 67
    "scala.collection.immutable.$colon$colon" = 68
    "scala.collection.mutable.ArrayBuffer" = 69
    "scala.collection.immutable.Vector" = 70
  }
}


custom-downing {
  stable-after = 20s

  majority-leader-auto-downing {
    majority-member-role = "worker"    # Must match akka.cluster.roles
    down-if-in-minority = true
    shutdown-actor-system-on-resolution = true
  }
}

kamon {
  modules {
    metriclog-reporter {
      enabled = false
      name = "MetricLog Reporter"
      description = "Log all Metrics"
      factory = "filodb.coordinator.KamonLogger$MetricsLogFactory"
    }
    spanlog-reporter {
      enabled = false
      name = "SpanLog Reporter"
      description = "Log all traced Spans"
      factory = "filodb.coordinator.KamonLogger$SpanLogFactory"
    }
    status-page {
      enabled = false
      name = "Status Page"
      description = "Exposes an embedded web server with a single page app displaying Kamon status information."
      factory = "kamon.status.page.StatusPage$Factory"
    }
    otel-trace-reporter {
      enabled = false
    }
  }

  otel.trace {
    # Hostname and port where the OTLP Server is running
    host = "localhost"
    port = 4317
  }

  prometheus.buckets {
    # Have more buckets, better resolution really helps esp with heatmaps
    default-buckets = [
      4,
      8,
      16,
      32,
      64,
      128,
      256,
      512,
      1024,
      2048,
      4096,
      8192,
      16384,
      32768,
      65536,
      131072,
      262144,
      524288
    ]

    # Start at 0.01ms so we can measure smaller timings
    time-buckets = [
      0.00001,
      0.000025,
      0.00005,
      0.0001,
      0.00025,
      0.0005,
      0.001,
      0.0025,
      0.005,
      0.01,
      0.025,
      0.05,
      0.1,
      0.25,
      0.5,
      1,
      2.5,
      5,
      10,
      25,
      50,
      100,
      250
    ]
  }
}
