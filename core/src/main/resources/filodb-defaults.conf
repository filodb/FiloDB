{
  filodb {
    cassandra {
      hosts = ["localhost"]
      port = 9042
      keyspace = "filodb"
      admin-keyspace = "filodb_admin"
      # username = "abc"
      # password = "xyz"
      # read-timeout = 12 s
      # connect-timeout = 5 s
      # default-consistency-level = QUORUM.  NOTE: see this link for the string values:
      # http://docs.datastax.com/en/drivers/java/2.1/com/datastax/driver/core/ConsistencyLevel.html

      # CQL CREATE KEYSPACE options.  You will want to change these for production.
      keyspace-replication-options = "{'class': 'SimpleStrategy', 'replication_factor': '1'}"
    }

    spark {
      # The port used by FiloDB coordinators on executor nodes and the driver to communicate with each
      # other via Akka clustering.  Leaving this commented out will default the port setting to 0, which
      # means find any free open port.
      # executor.port = 5444
      # driver.port = 5555
    }

    # Which column and metaStore to use, ie where to persist columnar chunks.
    # Valid values are "cassandra", "in-memory"
    # Only the filodb-spark library pays attention to this, not the CLI.
    store = "cassandra"

    columnstore {
      # Number of cache entries for the table cache
      tablecache-size = 50

      # Number of ChunkRowMap entries to cache
      segment-cache-size = 1000

      # Maximum number of chunks to flush at one time using one Unlogged Batch.
      chunk-batch-size = 16

      # Maximum number of partitions that can be fetched at once that will still fit in
      # one Spark partition/thread when using IN clause in query.
      # If number of partitions are more than this limit then full table scan is performed.
      inquery-partitions-limit = 12
    }

    memtable {
      # Uncomment to enable mmap-file based memtable for persistence and easy recovery upon crashes.
      # Defaults to in-memory DB only which is lost upon restarts/crashes, but easier for testing.
      # local-filename = "/tmp/filodb.memtable"

      # Maximum rows per dataset/version before ingestRows throws back a PleaseWait.
      max-rows-per-table = 200000

      # Number of rows in memtable before flushes start being triggered
      flush-trigger-rows = 100000

      # The minimum amount of free memory required to accept new rows into memtable
      min-free-mb = 300

      # Chunk size for FiloMemTable - determines minimum # of rows before serializing to chunks
      filo.chunksize = 1000

      # Maximum delay before new rows in FiloMemTable are flushed to WAL and turned into chunks.
      # The bigger this is, the less WAL disk activity, but also the longer before ingestRows
      # returns acknowledgement.  Think of it as max delay before new ingested rows are guaranteed
      # to be recovered.
      write.interval = 1 s

      # This is to start flush on memtable when there is no write activity
      noactivity.flush.interval = 10 s
    }

    metrics-logger {
      # Change to true to log internal metrics every tick (10s default, see Kamon.io docs)
      enabled = false

      # Filter different metrics to receive, can be a pattern, empty string to disable
      # ** = log everything / match all
      # trace = **
      # trace-segment = **
      # counter = **
      # gauge = **
      # histogram = **
      # min-max-counter = **
    }

    # Thread pool size (min and max) for filodb.core reprojection and I/O tasks.
    # NOTE: This definitely needs to be larger than the segment-batch-size, otherwise
    # all of the segments will be blocked waiting for reads.
    core-futures-pool-size = 128
    core-futures-max-pool-size = 128

    # Thread pool queue length.  When queue is full, tasks get run in calling thread.
    core-futures-queue-length = 1024

    reprojector {
      # Number of times to retry a segment write.  Exponential backoff included.
      retries = 3
      retry-base-timeunit = 5 s

      # Number of segments to flush at a time during reprojection.  Serves to throttle ingestion;
      # higher number uses more memory and causes more write contention to the ColumnStore, but may be
      # slightly more efficient.  If tight on memory, or seeing write timeouts, reduce this number.
      # Ingesting small-segment datasets may require this to be raised.
      segment-batch-size = 32
    }

    hive {
      # Uncomment the below to enable automatic syncing of FiloDB datasets into Hive Metastore external
      # tables so that one does not need to register tables manually with the Hive store.
      # FiloDB tables in the cassandra keyspace below will be synced to the Hive database name below.
      # database-name = "filodb"
    }
  }

  akka {
    # Use SLF4J for deployed environment logging
    loggers = ["akka.event.slf4j.Slf4jLogger"]
    loglevel = "INFO"
    actor {
      provider = "akka.cluster.ClusterActorRefProvider"
      warn-about-java-serializer-usage = off
      debug {
        # To enable logging of every Akka message received by the various actors, uncomment the lines below,
        # then change the loglevel above to DEBUG
        # receive = on
        # autoreceive = on
        lifecycle = on
      }
    }

    remote {
      log-remote-lifecycle-events = off
      netty.tcp {
        # Leave out the hostname, it will be automatically determined.
        # The Akka port will be overridden by filodb.spark.* settings
        port = 0
        send-buffer-size = 512000b
        receive-buffer-size = 512000b
        maximum-frame-size = 10 MiB
      }
    }

    cluster {
      auto-down-unreachable-after = 20s
      metrics.enabled = off
      failure-detector.acceptable-heartbeat-pause = 3s
      failure-detector.threshold = 12.0
    }
  }
}
