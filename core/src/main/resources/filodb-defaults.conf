filodb {
  # list of paths to dataset + ingestion config files
  dataset-configs = [ ]

  # As an alternative to dataset-configs, one can inline list each dataset/stream ingestion config
  # here, which might be more convenient in some cases than listing file paths (eg tests, reuse of shared configs).
  # This is only used if dataset-configs is empty.
  # inline-dataset-configs = [
  #   {
  #     dataset = "prometheus"
  #     definition { ... }
  #     sourceconfig { ... }
  #   },
  # ]
  inline-dataset-configs = []

  # Definition of cluster-wide partition key scheme.  The partition key defines each unique time series,
  # such as labels or tags, and is used for sharding time series across the cluster.
  # The below definition is standard for Prometheus schema.
  # Please do not modify unless you know what you are doing; some internal code esp in gateway module is sensitive
  # to the exact definitions.
  partition-schema {
    # Typical column types used: map, string.  Also possible: ts,long,double
    columns = ["metric:string", "tags:map"]

    # Predefined keys allow space to be saved for over the wire tags with the given keys
    # WARN: It is suggested to only ADD to predefined keys.
    #       Changing/Renaming existing keys will cause incorrect tags to be reported and probably require a clean wipe.
    predefined-keys = ["_ws_", "_ns_", "app", "__name__", "instance", "dc", "le", "job", "exporter"]

    options {
      # Copy tags can be used to create a new labelPair from one of the existing labelPair in a TimeSeries.
      # It honors the order in which the label keys are represented against a new key.
      # Eg: For copyTags `"_ns_" = [ "exporter", "job" ]`, when an incoming TimeSeries already has both `exporter`
      #     and `job`, then it will pickup only the value from `exporter` for `_ns_`.
      copyTags = {
        "_ns_" = [ "_ns", "exporter", "job" ]
      }
      ignoreShardKeyColumnSuffixes = { "__name__" = ["_bucket", "_count", "_sum"] }
      ignoreTagsOnPartitionKeyHash = ["le"]
      metricColumn = "metric"
      shardKeyColumns = [ "metric", "_ws_", "_ns_" ]
    }
  }

  # Definitions of possible data schemas to be used in all datasets
  # Each one must have a unique name and column schema.
  # FiloDB will refuse to start if the schema definitions have errors.  Use the validateSchemas CLI command to check.
  # Please do not modify unless you know what you are doing; some internal code esp in gateway module is sensitive
  # to the exact definitions.
  schemas {
    gauge {
      # Each column def is of name:type format.  Type may be ts,long,double,string,int
      # The first column must be ts or long
      columns = ["timestamp:ts", "value:double:detectDrops=false"]

      # Default column to query using PromQL
      value-column = "value"

      # Downsampling configuration.  See doc/downsampling.md
      downsamplers = [ "tTime(0)", "dMin(1)", "dMax(1)", "dSum(1)", "dCount(1)", "dAvg(1)" ]

      # If downsamplers are defined, then the downsample schema must also be defined
      downsample-schema = "ds-gauge"
    }

    # For metrics from Prom or other places without a known type, we treat them as gauges
    # The column name is different to be sure of no hash conflicts
    untyped {
      columns = ["timestamp:ts", "number:double"]
      value-column = "number"
      downsamplers = [ "tTime(0)", "dMin(1)", "dMax(1)", "dSum(1)", "dCount(1)", "dAvg(1)" ]
      downsample-schema = "ds-gauge"
    }

    prom-counter {
      columns = ["timestamp:ts", "count:double:detectDrops=true"]
      value-column = "count"
      # This is temporary until we add downsampling for counters as well
      downsamplers = [ "tTime(0)", "dMin(1)", "dMax(1)", "dSum(1)", "dCount(1)", "dAvg(1)" ]
      downsample-schema = "ds-gauge"
    }

    prom-histogram {
      columns = ["timestamp:ts",
                 "sum:double:detectDrops=true",
                 "count:double:detectDrops=true",
                 "h:hist:counter=true"]
      value-column = "h"
      downsamplers = []
    }

    # Used for downsampled gauge data
    ds-gauge {
      columns = [ "timestamp:ts", "min:double", "max:double", "sum:double", "count:double", "avg:double" ]
      value-column = "avg"
      downsamplers = []
    }
  }

  tasks {
    # Frequency with which new shard maps are published
    shardmap-publish-frequency = 5s

    # Internal task configs for handling lifecycle management and events
    timeouts {
      default = 8000ms
      initialization = 60s
      graceful-stop = 8000ms
      resolve-actor = 10s

      # The timeout before retrying status updates
      status-ack-timeout = 15s
    }
  }

  scheduler {
    enable-assertions = false
  }

  profiler {
    # Uncomment to enable profiling, using the filodb.standalone.SimpleProfiler class.
    #sample-rate = 10ms
    #report-interval = 60s
    #top-count = 50
    #out-file = "filodb.prof"
  }

  # # of shards each application/metric is spread out to = 2^spread
  spread-default = 1
  # default spread can be overriden for a specific sharding key combination.
  # Eg: If "__name__, _ns" are your sharding key, for a _ns "App-Test001" the spread can be overriden as follows:
  # spread-assignment = [ { _ws_ = demo, _ns_ = App-Test001, _spread_ = 5 } ]
  spread-assignment = []

  query {
    # Timeout for query engine subtree/ExecPlans for requests to sub nodes
    # Higher default until we have a way to really timeout query execution at leaves
    ask-timeout = 120 seconds

    stale-sample-after = 5 minutes

    # Maximum number of samples to return in a query
    sample-limit = 1000000

    # Minimum step required for a query
    min-step = 5 seconds

    # Parallelism (query threadpool per dataset) ... ceil(available processors * factor)
    threads-factor = 1.0

    # Maximum number of steps/windows to use the RangeVectorAggregator.fastReduce aggregators.  This aggregator
    # uses memory proportional to the # of windows, rather than the # of time series aggregated; it can speed up
    # high cardinality aggregations in particular.
    fastreduce-max-windows = 50

    # Enable faster rate/increase/delta calculations. Depends on drop detection in chunks (detectDrops=true)
    faster-rate = true
  }

  shard-manager {
    # Minimum time required between successive automatic shard reassignments done by ShardManager
    reassignment-min-interval = 2 hours
  }

  cassandra {
    hosts = ["localhost"]
    port = 9042
    keyspace = "filodb"
    admin-keyspace = "filodb_admin"
    # username = "abc"
    # password = "xyz"
    # read-timeout = 12 s
    # connect-timeout = 5 s
    # default-consistency-level = QUORUM.  NOTE: see this link for the string values:
    # http://docs.datastax.com/en/drivers/java/2.1/com/datastax/driver/core/ConsistencyLevel.html

    # Number of parallel chunkset writes at a time
    write-parallelism = 4

    # CQL CREATE KEYSPACE options.  You will want to change these for production.
    keyspace-replication-options = "{'class': 'SimpleStrategy', 'replication_factor': '1'}"

    # NONE, LZ4, SNAPPY.  Compression of CQL traffic over the network.  Turn on for remote clusters,
    # can help by up to 20-30% - unless you use lz4-chunk-compress, in which case leave this off
    cql-compression = "NONE"

    # Compress columnar chunks using LZ4 and store them over CQL that way.
    # Use this instead of cql-compression for the best read performance
    lz4-chunk-compress = false

    # See http://docs.datastax.com/en/cql/3.1/cql/cql_reference/compressSubprop.html for valid values;
    # Use "" to turn off compression.  For the main chunks table only.
    sstable-compression = "LZ4Compressor"

    # retry duration (including jitter) is configured to be little more than chunk-duration / groups-per-shard
    max-retry-attempts = 5
    retry-interval = 10s
    retry-interval-max-jitter = 10s

    ingestion-consistency-level = "ONE"
  }

  downsampler {

    # Name of the dataset from which to downsample
    # raw-dataset-name = "prometheus"

    # Raw schemas from which to downsample
    # raw-schema-names = [ "prometheus" ]

    # Resolutions for downsampled data
    # resolutions = [ 1 minute, 5 minutes ]

    # Retention of downsampled data for the corresponding resolution
    # ttls = [ 30 days, 183 days ]

    # Use to override cassandra Session Provider class name used by downsampler
    # cass-session-provider-fqcn = fqcn

    # Number of time series to operate on at one time. Reduce if there is much less memory available
    num-partitions-per-cass-write = 10000

    # This block memory is used for overflow of write buffers
    off-heap-block-memory-size = 500 MB

    # Configure to NumPartitionsPerCassWrite * (EstChunkSize * EstNumChunksPerPartition + PartKeySize) + buffer
    # 10000 * (1KB * 2 + 500 bytes) + buffer
    off-heap-native-memory-size = 500 MB

    # Amount of time to wait for a Cassandra write to finish before proceeding to next batch of partitions
    cassandra-write-timeout = 10.minutes

    # How much to increase userTime range by to formulate ingestionTime range to search for chunks to downsample.
    # Recommended to be at least twice chunkDuration of raw dataset to accommodate for early/late arriving data
    widen-ingestion-time-range-by = 2h

    # configure only these three items.
    downsample-store-config {
      # Amount of ingestion time to query to create one chunk of downsampled data
      flush-interval = 6h

      # number of items in the downsampled chunk
      max-chunks-size = 400

      # Write buffer size, in bytes, for blob columns (histograms, UTF8Strings).  Since these are variable data types,
      # we need a maximum size, not a maximum number of items.
      max-blob-buffer-size = 15000

      # not used
      shard-mem-size = 1MB
    }

  }

  spark {
    # The amount of time to wait for dataset creation, truncation, schema changes, etc.
    dataset-ops-timeout = 30s

    # The amount of time to wait for a dataset to finish flushing at the end of a DataFrame write
    flush-timeout = 5m

    # The port used by FiloDB coordinators on executor nodes and the driver to communicate with each
    # other via Akka clustering.  Leaving this commented out will default the port setting to 0, which
    # means find any free open port.
    # executor.port = 5444
    # driver.port = 5555
  }

  # Which MemStore, ChunkSink (for chunk persistence) and MetaStore to use
  # Should be the full class path / FQCN to an implementation of StoreFactory
  store-factory = "filodb.coordinator.TimeSeriesNullStoreFactory"

  columnstore {
    # Number of cache entries for the table cache
    tablecache-size = 50

    # Maximum number of partitions that can be fetched at once that will still fit in
    # one Spark partition/thread when using IN clause in query.
    # If number of partitions are more than this limit then full table scan is performed.
    inquery-partitions-limit = 12
  }

  memstore {
    # Parallelism of persistance flush tasks. Should never be greater than groups-per-shard
    flush-task-parallelism = 2

    # Minimum amount of memory (as a memory string, eg 2MB, 1GB) to maintain in the write buffers.
    # This is also memory for other time series data structures.  Falling below this number means data will be
    # evicted from memory and possibly lost new data if not enough eviction happens.
    # It should be large enough to accommodate a new partition key, a new OffheapLFSortedIDMap structure
    # (a couple hundred bytes) plus 1000 write buffers.
    min-write-buffers-free = 5MB
  }

  # for standalone worker cluster configuration, see akka-bootstrapper

  # dataset-definitions:
  # See FiloServer.scala for a way to automatically define datasets at startup

  hive {
    # Uncomment the below to enable automatic syncing of FiloDB datasets into Hive Metastore external
    # tables so that one does not need to register tables manually with the Hive store.
    # FiloDB tables in the cassandra keyspace below will be synced to the Hive database name below.
    # database-name = "filodb"
  }
}

query-actor-mailbox {
  mailbox-type = "filodb.coordinator.QueryActorMailbox"
}

# Configuration for the open-source ingestion gateway
gateway {
  # TCP Port for Influx Line Protocol incoming data
  influx-port = 8007

  # Number of threads or parallel tasks serializing input records to container format for Kafka/sink
  producer-parallelism = 16

  # Minimum size of queue for each shard's records.  Must be a power of 2
  min-queue-size = 256

  # Maximum size of queue for each shard's records.  Must be a power of 2.  When the max queue size is reached
  # then the connection will stop accepting incoming records.
  max-queue-size = 16384

  # Amount of sleep the ingesting TCP server does when the queue for a given shard is full
  queue-full-wait = 100ms

  tcp {
    ssl-enabled = false
    netty-receive-buffer-size = 1048576
    netty-send-buffer-size = 1048576
  }
}

akka {

  test.single-expect-default = 10s

  extensions = ["filodb.coordinator.FilodbCluster",
                "com.romix.akka.serialization.kryo.KryoSerializationExtension$"]

  # Use SLF4J for deployed environment logging
  loggers = ["akka.event.slf4j.Slf4jLogger"]

  # will filter the log events using the backend configuration
  # (e.g. logback.xml) before they are published to the event bus.
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  loglevel = "INFO"
  actor {

    #https://doc.akka.io/docs/akka/2.5/project/migration-guide-2.4.x-2.5.x.html#additional-serialization-bindings
    # Set this to off to disable serialization-bindings defined in
    # additional-serialization-bindings. That should only be needed
    # for backwards compatibility reasons.
    enable-additional-serialization-bindings = off

    provider = "akka.cluster.ClusterActorRefProvider"
    warn-about-java-serializer-usage = off
    debug {
      # To enable logging of every Akka message received by the various actors, uncomment the lines below,
      # then change the loglevel above to DEBUG
      # receive = on
      # autoreceive = on
      lifecycle = on
    }

    # For details of kryo section see https://github.com/romix/akka-kryo-serialization
    kryo {
      implicit-registration-logging = "false"
      kryo-trace = "false"

      kryo-custom-serializer-init = "filodb.coordinator.client.KryoInit"

      # Make the buffer size bigger as we send out chunks quite often
      buffer-size = 65536

      # automatic means fall back to FQCN's if class is not pre-registered.  This is safer.
      idstrategy = "automatic"

      mappings {
        # Set standard IDs here from say Scala up to 99.
        # DO NOT INCLUDE FiloDB classes here.  Those are better to put in Serializer.scala so they can be typechecked.
        "scala.Some" = 64
        "scala.Tuple2" = 65
        "scala.None$" = 66
        "scala.collection.immutable.Nil$" = 67
        "scala.collection.immutable.$colon$colon" = 68
        "scala.collection.mutable.ArrayBuffer" = 69
        "scala.collection.immutable.Vector" = 70
      }
    }

    serializers {
      kryo = "com.romix.akka.serialization.kryo.KryoSerializer"
    }

    serialization-bindings {
      "filodb.coordinator.client.IngestionCommands$IngestRows" = kryo
      "filodb.query.QueryCommand" = kryo
      "filodb.coordinator.client.QueryResponse" = kryo
      "filodb.core.query.Result" = kryo
      "filodb.coordinator.client.DatasetCommands$CreateDataset" = kryo
      "filodb.coordinator.StatusActor$EventEnvelope" = kryo
      "filodb.coordinator.StatusActor$StatusAck" = kryo
      "filodb.coordinator.CurrentShardSnapshot" = kryo

      "filodb.query.QueryResult" = kryo
      "filodb.query.QueryError" = kryo
      "filodb.query.exec.ExecPlan" = kryo
      "filodb.query.LogicalPlan" = kryo
    }

    # Reduce the number of threads used by default by the fork-join pool, as it's not really doing much work.
    default-dispatcher.fork-join-executor {
      parallelism-factor = 2.0
      parallelism-max = 32
    }

    # Just the defaults to start with. TODO optimize and pick the executor needed.
    shard-status-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
      # Configuration for the fork join pool
      fork-join-executor {
        # Min number of threads to cap factor-based parallelism number to
        parallelism-min = 2
        # Parallelism (threads) ... ceil(available processors * factor)
        parallelism-factor = 2.0
        # Max number of threads to cap factor-based parallelism number to
        parallelism-max = 10
      }
      # Throughput defines the maximum number of messages to be
      # processed per actor before the thread jumps to the next actor.
      # Set to 1 for as fair as possible.
      throughput = 100
    }
  }

  remote {
    log-remote-lifecycle-events = off
    netty {

      #https://doc.akka.io/docs/akka/2.5/project/migration-guide-2.4.x-2.5.x.html#mutual-tls-authentication-now-required-by-default-for-netty-based-ssl-transport
      ssl.require-mutual-authentication = off

      tcp {
        # Leave out the hostname, it will be automatically determined.
        # The Akka port will be overridden by filodb.spark.* settings
        port = 0
        send-buffer-size = 1024000b
        receive-buffer-size = 1024000b
        maximum-frame-size = 25 MiB
      }
    }
  }

  cluster {
    roles = [worker]

    # If a join request fails it will be retried after this period. Disable join retry by specifying "off".
    retry-unsuccessful-join-after = 10s

    # Auto downing is turned off by default.  See
    # http://doc.akka.io/docs/akka/2.3.16/scala/cluster-usage.html#Automatic_vs__Manual_Downing
    # Instead, we use this repo to provide smarter downing solutions:
    # https://github.com/TanUkkii007/akka-cluster-custom-downing
    # Note that this strategy should work well but for fixed size clusters QuorumLeader might be better
    downing-provider-class = "tanukki.akka.cluster.autodown.MajorityLeaderAutoDowning"

    metrics.enabled = off
    failure-detector {
      heartbeat-interval = 5s
      acceptable-heartbeat-pause = 15s
      threshold = 12.0
      expected-response-after = 5s
    }
  }

  # Be sure to terminate/exit JVM process after Akka shuts down.  This is important for the
  # custom downing provider's split brain resolution to work properly.  Basically, the minority
  # group will shut down itself and exit the process, helping to bring newer nodes online.
  coordinated-shutdown.exit-jvm = on
}


custom-downing {
  stable-after = 20s

  majority-leader-auto-downing {
    majority-member-role = "worker"    # Must match akka.cluster.roles
    down-if-in-minority = true
    shutdown-actor-system-on-resolution = true
  }
}

kamon.zipkin {
  max.requests = 128
  message.max.bytes = 262144
}

kamon {
  prometheus.buckets {
    # Have more buckets, better resolution really helps esp with heatmaps
    default-buckets = [
      4,
      8,
      16,
      32,
      64,
      128,
      256,
      512,
      1024,
      2048,
      4096,
      8192,
      16384,
      32768,
      65536,
      131072,
      262144,
      524288
    ]

    # Start at 0.01ms so we can measure smaller timings
    time-buckets = [
      0.00001,
      0.000025,
      0.00005,
      0.0001,
      0.00025,
      0.0005,
      0.001,
      0.0025,
      0.005,
      0.01,
      0.025,
      0.05,
      0.1,
      0.25,
      0.5,
      1,
      2.5,
      5,
      10,
      25,
      50,
      100,
      250
    ]
  }
}
