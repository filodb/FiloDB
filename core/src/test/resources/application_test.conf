filodb {
  dataset-configs = [ ]

  cassandra {
    hosts = ["localhost"]
    port = 9042
    keyspace = "unittest"
    admin-keyspace = "unittest_admin"
    keyspace-replication-options = "{'class': 'SimpleStrategy', 'replication_factor': '1'}"
    lz4-chunk-compress = false
    sstable-compression = "LZ4Compressor"
    write-parallelism = 4
    max-retry-attempts = 5
    retry-interval = 10s
    retry-interval-max-jitter = 10s
    ingestion-consistency-level = "ONE"
    checkpoint-read-consistency-level = "LOCAL_QUORUM"
    default-read-consistency-level = "LOCAL_ONE"
    pk-by-updated-time-table-num-splits = 200
    write-time-index-ttl = 1 day
    create-tables-enabled = true
    num-token-range-splits-for-scans = 2
    index-scan-parallelism-per-shard = 2
  }

  shard-manager {
    reassignment-min-interval = 2 hours
  }

  store = "timeseries-null"

  tasks {
    shardmap-publish-frequency = 1s
  }

  quotas {
    default = 1000000
  }


  columnstore {
    # Number of cache entries for the table cache
    tablecache-size = 50

    # Maximum number of partitions that can be fetched at once that will still fit in
    # one Spark partition/thread when using IN clause in query.
    # If number of partitions are more than this limit then full table scan is performed.
    inquery-partitions-limit = 12
  }

  spark.dataset-ops-timeout = 15s

  memstore {
    flush-task-parallelism = 1
    ensure-block-memory-headroom-percent = 5
    ensure-tsp-count-headroom-percent = 5
    ensure-native-memory-headroom-percent = 5
    max-partitions-on-heap-per-shard = 10000
    ingestion-buffer-mem-size = 80MB
    track-queries-holding-eviction-lock = false
    index-faceting-enabled-shard-key-labels = true
    index-faceting-enabled-for-all-labels = true
  }

  tasks {
    # Internal task configs for handling lifecycle management and events
    timeouts {
      default = 30s
      initialization = 60s
      graceful-stop = 30s
      status-ack-timeout = 10s
    }
  }
  query {
    ask-timeout = 10 seconds
    stale-sample-after = 5 minutes
    sample-limit = 1000000
    min-step = 1 ms
    faster-rate = true
    fastreduce-max-windows = 50
    translate-prom-to-filodb-histogram = true
    parser = "antlr"
    enforce-result-byte-limit = true
    routing {
      # not currently used
    }
    allow-partial-results-metadataquery = true
    allow-partial-results-rangequery = false
  }

  spread-default = 1

  shard-key-level-ingestion-metrics-enabled = true
  cluster-type = "raw"
  deployment-partition-name = "local"

  spread-assignment = [
    {
      _ws_ = "demo",
      _ns_ = "App-0",
      _spread_ = 2
    }
  ]
}

query-actor-mailbox {
  mailbox-type = "filodb.coordinator.QueryActorMailbox"
}

akka {
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  # loglevel = "DEBUG"
  actor {
    provider = "akka.cluster.ClusterActorRefProvider"
    warn-about-java-serializer-usage = off
    # Make sure all messages are serialized in tests
    # serialize-messages = on
    # Same thing
    # serialize-creators = on
    debug {
      receive = on
      autoreceive = on
      # lifecycle = on
    }
  }

  remote {
    log-remote-lifecycle-events = off
    netty.tcp {
      # Leave out the hostname, it will be automatically determined.
      # The Akka port will be overridden by filodb.spark.* settings
      port = 0
      send-buffer-size = 512000b
      receive-buffer-size = 512000b
      maximum-frame-size = 10 MiB
      connection-timeout = 30s
    }
  }

  testconductor {
    # Longer timeout for reaching a barrier, esp for TravisCI / Jenkins etc.
    barrier-timeout = 5m
  }

  cluster {
    roles = [worker]
    # don't use sigar for tests, native lib not in path
    metrics.collector-class = akka.cluster.JmxMetricsCollector
  }

  # Don't terminate ActorSystem via CoordinatedShutdown in tests
  # Otherwise, the SBT forked process for tests will die prematurely
  #coordinated-shutdown.terminate-actor-system = off
  cluster.run-coordinated-shutdown-when-down = off
  coordinated-shutdown.exit-jvm = off
}

kamon {
  modules {
    zipkin-reporter.enabled = false
    prometheus-reporter.enabled = false
    status-page.enabled = false
    otel-trace-reporter.enabled = false
  }
}