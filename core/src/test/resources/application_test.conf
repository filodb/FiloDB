filodb {
  dataset-configs = [ ]

  partition = "test-partition"

  partition-schema {
    columns = ["_metric_:string", "tags:map"]

    predefined-keys = []

    options {
      copyTags = {
        "_ns_" = ["_ns", "exporter", "job"]
      }
      ignoreShardKeyColumnSuffixes = {"_metric_" = ["_bucket", "_count", "_sum"]}
      ignoreTagsOnPartitionKeyHash = ["le"]
      metricColumn = "_metric_"
      shardKeyColumns = ["_ws_", "_ns_", "_metric_"]
      multiColumnFacets = {}
    }
  }

  schemas {
    untyped {
      columns = ["timestamp:ts", "number:double"]
      value-column = "number"
      downsamplers = []
    }
  }

  cassandra {
    hosts = ["localhost"]
    port = 9042
    keyspace = "unittest"
    admin-keyspace = "unittest_admin"
    keyspace-replication-options = "{'class': 'SimpleStrategy', 'replication_factor': '1'}"
    lz4-chunk-compress = false
    sstable-compression = "LZ4Compressor"
    write-parallelism = 4
    max-retry-attempts = 5
    retry-interval = 10s
    retry-interval-max-jitter = 10s
    ingestion-consistency-level = "ONE"
    checkpoint-read-consistency-level = "LOCAL_QUORUM"
    default-read-consistency-level = "LOCAL_ONE"
    pk-by-updated-time-table-num-splits = 200
    write-time-index-ttl = 1 day
    create-tables-enabled = true
    num-token-range-splits-for-scans = 2
    index-scan-parallelism-per-shard = 2
    part-keys-v2-table-enabled = false
    pk-v2-table-num-buckets = 100
  }

  grpc {
    start-grpc-service = false
    bind-grpc-port = 8888
    # See https://github.com/grpc/grpc/blob/master/doc/keepalive.md for more details of these parameters
    idle-timeout-seconds = 3600   # 1 hour
    keep-alive-time-seconds = 60  # every minute
    keep-alive-timeout-seconds = 60  # keep alive ack will timeout and connection closed  if no response is seen in this time
    max-inbound-message-size = 104857600
  }

  shard-manager {
    reassignment-min-interval = 2 hours
  }

  store = "timeseries-null"

  tasks {
    shardmap-publish-frequency = 1s
  }

  quotas {
    default = 1000000
  }


  columnstore {
    # Number of cache entries for the table cache
    tablecache-size = 50

    # Maximum number of partitions that can be fetched at once that will still fit in
    # one Spark partition/thread when using IN clause in query.
    # If number of partitions are more than this limit then full table scan is performed.
    inquery-partitions-limit = 12
  }

  spark.dataset-ops-timeout = 15s

  memstore {
    memory-alloc {
      automatic-alloc-enabled = false
      available-memory-bytes = 1GB
      os-memory-needs = 500MB
      lucene-memory-percent = 20
      native-memory-manager-percent = 20
      block-memory-manager-percent = 60
    }

    flush-task-parallelism = 1
    ensure-block-memory-headroom-percent = 5
    ensure-tsp-count-headroom-percent = 5
    ensure-native-memory-headroom-percent = 5
    max-partitions-on-heap-per-shard = 10000
    ingestion-buffer-mem-size = 80MB
    track-queries-holding-eviction-lock = false
    index-faceting-enabled-shard-key-labels = true
    index-faceting-enabled-for-all-labels = true
    disable-index-caching = false

  }

  tasks {
    # Internal task configs for handling lifecycle management and events
    timeouts {
      default = 30s
      initialization = 60s
      graceful-stop = 30s
      status-ack-timeout = 10s
    }
  }
  query {
    streaming-query-results-enabled = false
    num-rvs-per-result-message = 25
    ask-timeout = 10 seconds
    stale-sample-after = 5 minutes
    sample-limit = 1000000
    min-step = 1 ms
    faster-rate = true
    fastreduce-max-windows = 50
    translate-prom-to-filodb-histogram = true
    parser = "antlr"
    enforce-result-byte-limit = true
    container-size-overrides {
        filodb-query-exec-aggregate-large-container                    = 65536
        filodb-query-exec-metadataexec                                 = 65536
    }
    grpc {
          partitions-deny-list = ""
       }
    routing {
      enable-remote-raw-exports = false
      max-time-range-remote-raw-export = 30 minutes
      enable-approximate-equals-in-stitch = true
      period-of-uncertainty-ms = 0 minutes
    }
    allow-partial-results-metadataquery = true
    allow-partial-results-rangequery = false
    circuit-breaker {
      enabled = false
      open-when-num-failures = 5
      reset-timeout = 15 seconds
      exp-backoff-factor = 1.5
      max-reset-timeout = 5 minutes
    }
  }

  spread-default = 1

  shard-key-level-ingestion-metrics-enabled = true
  cluster-type = "raw"
  deployment-partition-name = "local"

  spread-assignment = [
    {
      _ws_ = "demo",
      _ns_ = "App-0",
      _spread_ = 2
    }
  ]
}

query-actor-mailbox {
  mailbox-type = "filodb.coordinator.QueryActorMailbox"
}

akka {
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  # loglevel = "DEBUG"
  actor {
    provider = "akka.cluster.ClusterActorRefProvider"
    warn-about-java-serializer-usage = off
    # Make sure all messages are serialized in tests
    # serialize-messages = on
    # Same thing
    # serialize-creators = on
    debug {
      receive = on
      autoreceive = on
      # lifecycle = on
    }
  }

  remote {
    log-remote-lifecycle-events = off
    netty.tcp {
      # Leave out the hostname, it will be automatically determined.
      # The Akka port will be overridden by filodb.spark.* settings
      port = 0
      send-buffer-size = 512000b
      receive-buffer-size = 512000b
      maximum-frame-size = 10 MiB
      connection-timeout = 30s
    }
  }

  testconductor {
    # Longer timeout for reaching a barrier, esp for TravisCI / Jenkins etc.
    barrier-timeout = 5m
  }

  cluster {
    roles = [worker]
    # don't use sigar for tests, native lib not in path
    metrics.collector-class = akka.cluster.JmxMetricsCollector
  }

  # Don't terminate ActorSystem via CoordinatedShutdown in tests
  # Otherwise, the SBT forked process for tests will die prematurely
  #coordinated-shutdown.terminate-actor-system = off
  cluster.run-coordinated-shutdown-when-down = off
  coordinated-shutdown.exit-jvm = off
}

kamon {
  modules {
    zipkin-reporter.enabled = false
    prometheus-reporter.enabled = false
    status-page.enabled = false
    otel-trace-reporter.enabled = false
  }
}