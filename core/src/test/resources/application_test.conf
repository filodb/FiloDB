{
  filodb {
    cassandra {
      hosts = ["localhost"]
      port = 9042
      keyspace = "unittest"
      admin-keyspace = "unittest"
      keyspace-replication-options = "{'class': 'SimpleStrategy', 'replication_factor': '1'}"
      lz4-chunk-compress = false
      sstable-compression = "LZ4Compressor"
    }

    store = "in-memory"

    columnstore {
      # Number of cache entries for the table cache
      tablecache-size = 50

      # Maximum number of partitions that can be fetched at once that will still fit in
      # one Spark partition/thread when using IN clause in query.
      # If number of partitions are more than this limit then full table scan is performed.
      inquery-partitions-limit = 12
    }

    spark.dataset-ops-timeout = 15s

    memtable {
      # Uncomment to enable mmap-file based memtable for persistence and easy recovery upon crashes.
      # Defaults to in-memory DB only which is lost upon restarts/crashes, but easier for testing.
      # local-filename = "/tmp/filodb.memtable"

      # Maximum rows per dataset/version before ingestRows throws back a PleaseWait.
      max-rows-per-table = 2000

      # Number of rows in memtable before flushes start being triggered
      flush-trigger-rows = 5000

      # Set the free memory requirement much lower for running tests
      min-free-mb = 10

      filo.chunksize = 100

      # Make this high enough that tests will require manual flushing usually
      write.interval = 5 s

      # This is to start flush on memtable when there is no write activity
      noactivity.flush.interval = 10 s
    }

    write-ahead-log {
      # Location to store memtable write ahead log files for each dataset
      memtable-wal-dir = /tmp/filodb/wal

      # Minimum size of the Memory mapped ByteBuffer allocated for write ahead log file in bytes
      mapped-byte-buffer-size = 128000

      # Change to true to reload existing WAL files during ingestion
      reload-wal-enabled = true

      # Change to true to create WAL files during ingestion
      write-ahead-log-enabled = true
    }

    reprojector {
      # Number of times to retry a segment write.  Exponential backoff included.
      retries = 2
      retry-base-timeunit = 1 s

      # Number of ChunkRowMap entries to cache
      segment-cache-size = 1000

      segment-batch-size = 8

      bulk-write-mode = false
    }

    # Thread pool size for filodb.core reprojection and I/O tasks.
    core-futures-pool-size = 8
    core-futures-max-pool-size = 8

    # Thread pool queue length.  When queue is full, tasks get run in calling thread.
    core-futures-queue-length = 64
  }

  akka {
    loggers = ["akka.event.slf4j.Slf4jLogger"]
    # loglevel = "DEBUG"
    actor {
      provider = "akka.cluster.ClusterActorRefProvider"
      warn-about-java-serializer-usage = off
      debug {
        receive = on
        autoreceive = on
        # lifecycle = on
      }

      serializers {
        filoingest = "filodb.coordinator.IngestRowsSerializer"
      }

      serialization-bindings {
        "filodb.coordinator.IngestionCommands$IngestRows" = filoingest
      }
    }

    remote {
      log-remote-lifecycle-events = off
      netty.tcp {
        # Leave out the hostname, it will be automatically determined.
        # The Akka port will be overridden by filodb.spark.* settings
        port = 0
        send-buffer-size = 512000b
        receive-buffer-size = 512000b
        maximum-frame-size = 10 MiB
      }
    }

    cluster {
      roles = [executor]
      # don't use sigar for tests, native lib not in path
      metrics.collector-class = akka.cluster.JmxMetricsCollector
    }
  }
}
