filodb {

  # Definition of cluster-wide partition key scheme.  The partition key defines each unique time series,
  # such as labels or tags, and is used for sharding time series across the cluster.
  # The below definition is standard for Prometheus schema
  partition-schema {
    # Typical column types used: map, string.  Also possible: ts,long,double
    columns = ["tags:map"]

    # Predefined keys allow space to be saved for over the wire tags with the given keys
    predefined-keys = ["_ns", "app", "__name__", "instance", "dc", "le"]

    options {
      copyTags = {}
      ignoreShardKeyColumnSuffixes = { "__name__" = ["_bucket", "_count", "_sum"] }
      ignoreTagsOnPartitionKeyHash = ["le"]
      metricColumn = "__name__"
      shardKeyColumns = [ "__name__", "_ns" ]
    }
  }

  # Definitions of possible data schemas to be used in all datasets
  # Each one must have a unique name and column schema.
  # FiloDB will refuse to start if the schema definitions have errors.  Use the validateSchemas CLI command to check.
  schemas {
    prometheus {
      # Each column def is of name:type format.  Type may be ts,long,double,string,int
      # The first column must be ts or long
      columns = ["timestamp:ts", "value:double:detectDrops=true"]

      # Default column to query using PromQL
      value-column = "value"

      # Downsampling configuration.  See doc/downsampling.md
      downsamplers = [ "tTime(0)", "dMin(1)", "dMax(1)", "dSum(1)", "dCount(1)", "dAvg(1)" ]

      # If downsamplers are defined, then the downsample schema must also be defined
      downsample-schema = "prom-ds-gauge"
    }

    # Used for downsampled gauge data
    prom-ds-gauge {
      columns = [ "timestamp:ts", "min:double", "max:double", "sum:double", "count:double", "avg:double" ]
      value-column = "avg"
      downsamplers = []
    }
  }

  downsampler {

    raw-dataset-name = "prometheus"

    raw-schema-names = [ "prometheus" ]

    resolutions = [ 1 minute, 5 minutes ]

    ttls = [ 30 days, 183 days ]

    num-partitions-per-cass-write = 10000

    off-heap-block-memory-size = 2GB

    off-heap-native-memory-size = 1GB

    downsample-store-config {
      # Interval it takes to flush ALL time series in a shard.  This time is further divided by groups-per-shard
      flush-interval = 6h

      # TTL for on-disk / C* data.  Data older than this may be purged.
      disk-time-to-live = 24 hours

      # amount of time paged chunks should be retained in memory.
      # We need to have a minimum of x hours free blocks or else init won't work.
      demand-paged-chunk-retention-period = 24 hours

      max-chunks-size = 400

      # Write buffer size, in bytes, for blob columns (histograms, UTF8Strings).  Since these are variable data types,
      # we need a maximum size, not a maximum number of items.
      max-blob-buffer-size = 15000

      # Number of bytes of offheap mem to allocate to chunk storage in each shard.  Ex. 1000MB, 1G, 2GB
      # Assume 5 bytes per sample, should be roughly equal to (# samples per time series) * (# time series)
      shard-mem-size = 512MB

      # Number of bytes of offheap mem to allocate to write buffers for all shards.  Ex. 1000MB, 1G, 2GB
      ingestion-buffer-mem-size = 200MB

      # Maximum numer of write buffers to retain in each shard's WriteBufferPool.  Any extra buffers are released
      # back to native memory, which helps memory reuse.
      # max-buffer-pool-size = 10000

      # Number of time series to evict at a time.
      # num-partitions-to-evict = 1000

      # Number of subgroups within each shard.  Persistence to a ChunkSink occurs one subgroup at a time, as does
      # recovery from failure.  This many batches of flushes must occur to cover persistence of every partition
      groups-per-shard = 20

      # Use a "MultiPartitionScan" or Cassandra MULTIGET for on-demand paging. Might improve performance.
      multi-partition-odp = false

      # Amount of parallelism during on-demand paging
      # demand-paging-parallelism = 4

      # Number of retries for IngestionSource/Kafka initialization
      # failure-retries = 3

      # Amount of time to delay before retrying
      # retry-delay = 15s

      # Capacity of Bloom filter used to track evicted partitions.
      # Tune this based on how much time series churn is expected before a FiloDB node
      # will be restarted for upgrade/maintenance. Do not take into account churn created by
      # time series that are purged due to retention. When a time series is not ingesting for retention
      # period, it is purged, not evicted. Purged PartKeys are not added to Bloom Filter.
      #
      # To calculate Bloom Filter size:
      # console> BloomFilter[String](5000000, falsePositiveRate = 0.01).numberOfBits
      # res9: Long = 47925292
      # Thats about 6MB
      evicted-pk-bloom-filter-capacity = 50000

      # Uncomment to log at DEBUG ingested samples matching these filters on Partition Key columns
      # Only works for StringColumn fields in the partition key. Scope may be expanded later.
      # trace-filters = {
      #   metric = "bad-metric-to-log"
      # }

    }

  }


  cassandra {
    hosts = ["localhost"]
    port = 9042
    keyspace = "filodb"
    admin-keyspace = "filodb_admin"
    # username = "abc"
    # password = "xyz"
    # read-timeout = 12 s
    # connect-timeout = 5 s
    # default-consistency-level = QUORUM.  NOTE: see this link for the string values:
    # http://docs.datastax.com/en/drivers/java/2.1/com/datastax/driver/core/ConsistencyLevel.html

    # Number of parallel chunkset writes at a time
    write-parallelism = 4

    # CQL CREATE KEYSPACE options.  You will want to change these for production.
    keyspace-replication-options = "{'class': 'SimpleStrategy', 'replication_factor': '1'}"

    # NONE, LZ4, SNAPPY.  Compression of CQL traffic over the network.  Turn on for remote clusters,
    # can help by up to 20-30% - unless you use lz4-chunk-compress, in which case leave this off
    cql-compression = "NONE"

    # Compress columnar chunks using LZ4 and store them over CQL that way.
    # Use this instead of cql-compression for the best read performance
    lz4-chunk-compress = false

    # See http://docs.datastax.com/en/cql/3.1/cql/cql_reference/compressSubprop.html for valid values;
    # Use "" to turn off compression.  For the main chunks table only.
    sstable-compression = "LZ4Compressor"

    # retry duration (including jitter) is configured to be little more than chunk-duration / groups-per-shard
    max-retry-attempts = 5
    retry-interval = 10s
    retry-interval-max-jitter = 10s

    ingestion-consistency-level = "ONE"
  }

  columnstore {
    # Number of cache entries for the table cache
    tablecache-size = 50

    # Maximum number of partitions that can be fetched at once that will still fit in
    # one Spark partition/thread when using IN clause in query.
    # If number of partitions are more than this limit then full table scan is performed.
    inquery-partitions-limit = 12
  }

}