    dataset = "prometheus"

    # Should not change once dataset has been set up on the server and data has been persisted to cassandra
    definition {
      partition-columns = ["tags:map"]
      data-columns = ["timestamp:ts", "value:double"]
      row-key-columns = [ "timestamp" ]
      downsamplers = [ "tTime(0)", "dMin(1)", "dMax(1)", "dSum(1)", "dCount(1)"]
    }

    # Should not change once dataset has been set up on the server and data has been persisted to cassandra
    options {
      shardKeyColumns = [ "__name__", "_ns" ]
      ignoreShardKeyColumnSuffixes = { "__name__" = ["_bucket", "_count", "_sum"] }
      valueColumn = "value"
      metricColumn = "__name__"
      ignoreTagsOnPartitionKeyHash = [ "le" ]
      copyTags = { }
    }

    # Should not change once dataset has been set up on the server and data has been persisted to cassandra
    num-shards = 4


    min-num-nodes = 2
    # Length of chunks to be written, roughly
    sourcefactory = "filodb.kafka.KafkaIngestionStreamFactory"

    sourceconfig {
      # Required FiloDB configurations
      filo-topic-name = "timeseries-dev"

      # Standard kafka configurations, e.g.
      # This accepts both the standard kafka value of a comma-separated
      # string and a Typesafe list of String values
      # EXCEPT: do not populate value.deserializer, as the Kafka format is fixed in FiloDB to be messages of RecordContainer's
      bootstrap.servers = "localhost:9092"
      group.id = "filo-db-timeseries-ingestion"

      # Values controlling in-memory store chunking, flushing, etc.
      store {
        # Interval it takes to flush ALL time series in a shard.  This time is further divided by groups-per-shard
        flush-interval = 1h

        # TTL for on-disk / C* data.  Data older than this may be purged.
        disk-time-to-live = 24 hours

        # amount of time paged chunks should be retained in memory.
        # We need to have a minimum of x hours free blocks or else init won't work.
        demand-paged-chunk-retention-period = 12 hours

        max-chunks-size = 400

        # Write buffer size, in bytes, for blob columns (histograms, UTF8Strings).  Since these are variable data types,
        # we need a maximum size, not a maximum number of items.
        max-blob-buffer-size = 15000

        # Number of bytes of offheap mem to allocate to chunk storage in each shard.  Ex. 1000MB, 1G, 2GB
        # Assume 5 bytes per sample, should be roughly equal to (# samples per time series) * (# time series)
        shard-mem-size = 512MB

        # Number of bytes of offheap mem to allocate to write buffers for all shards.  Ex. 1000MB, 1G, 2GB
        ingestion-buffer-mem-size = 200MB

        # Maximum numer of write buffers to retain in each shard's WriteBufferPool.  Any extra buffers are released
        # back to native memory, which helps memory reuse.
        # max-buffer-pool-size = 10000

        # Number of time series to evict at a time.
        # num-partitions-to-evict = 1000

        # Number of subgroups within each shard.  Persistence to a ChunkSink occurs one subgroup at a time, as does
        # recovery from failure.  This many batches of flushes must occur to cover persistence of every partition
        groups-per-shard = 20

        # Use a "MultiPartitionScan" or Cassandra MULTIGET for on-demand paging. Might improve performance.
        multi-partition-odp = false

        # Amount of parallelism during on-demand paging
        # demand-paging-parallelism = 4

        # Number of retries for IngestionSource/Kafka initialization
        # failure-retries = 3

        # Amount of time to delay before retrying
        # retry-delay = 15s

        # Capacity of Bloom filter used to track evicted partitions.
        # Tune this based on how much time series churn is expected before a FiloDB node
        # will be restarted for upgrade/maintenance. Do not take into account churn created by
        # time series that are purged due to retention. When a time series is not ingesting for retention
        # period, it is purged, not evicted. Purged PartKeys are not added to Bloom Filter.
        #
        # To calculate Bloom Filter size:
        # console> BloomFilter[String](5000000, falsePositiveRate = 0.01).numberOfBits
        # res9: Long = 47925292
        # Thats about 6MB
        evicted-pk-bloom-filter-capacity = 50000

        # Uncomment to log at DEBUG ingested samples matching these filters on Partition Key columns
        # Only works for StringColumn fields in the partition key. Scope may be expanded later.
        # trace-filters = {
        #   metric = "bad-metric-to-log"
        # }
      }
      downsample {
        # can be disabled by setting this flag to false
        enabled = true
        # array of integers representing one or more downsample intervals in millisecond
        resolutions-ms = [ 60000 ]
        # class implementing the dispatch of downsample metrics to another dataset
        publisher-class = "filodb.kafka.KafkaDownsamplePublisher"
        publisher-config {
          # kafka properties that will be used for the producer
          kafka {
            bootstrap.servers = "localhost:9092"
            group.id = "filo-db-timeseries-downsample"
            linger.ms=1000
            batch.size=786432 // three times record container size
            acks = "-1"
            retries = 3
            max.in.flight.requests.per.connection=1
          }
          # map of millisecond resolution to the kafka topic for publishing downsample data
          # should have one topic per defined resolution above
          topics {
            60000 = "timeseries-dev-ds-1m"
          }
        }
      }
    }