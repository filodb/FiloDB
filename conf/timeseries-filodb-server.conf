dataset-prometheus = { include required("timeseries-dev-source.conf") }

filodb {
  v2-cluster-enabled = true
  cluster-discovery {
    num-nodes = 2
    failure-detection-interval = 20s
    host-list = [
       "127.0.0.1:2552",
       "127.0.0.1:3552"
    ]
  }

  store-factory = "filodb.cassandra.CassandraTSStoreFactory"

  cassandra {
    hosts = "localhost"
    port = 9042
    partition-list-num-groups = 1
    create-tables-enabled = true
  }

  inline-dataset-configs = [ ${dataset-prometheus} ]

  memstore {
    # Number of bytes of offheap mem to allocate to write buffers for all shards.  Ex. 1000MB, 1G, 2GB
    # Note: this memory is shared across all configued datasets on a node.
    ingestion-buffer-mem-size = 200MB
  }

  quotas {
    prometheus {
      defaults = [100, 500, 10000, 100000]
      custom = [
        {
          shardKeyPrefix = ["demo", "App-0", "heap_usage0"]
          quota = 100
        },
        {
          shardKeyPrefix = ["demo"]
          quota = 10
        }
      ]
    }
  }

  spread-default = 1

  # Override default spread for application using override block which will have non metric shard keys and spread.
  spread-assignment = [
    {
      _ws_ = demo,
      _ns_ = App-0,
      _spread_ = 2
    },
    {
      _ws_ = demo,
      _ns_ = App-5,
      _spread_ = 0
    }
  ]

  scheduler {
    enable-assertions = true
  }

  downsampler {
    raw-dataset-name = "prometheus"
    should-sleep-for-metrics-flush = false
  }
  ds-index-job {
    raw-dataset-name = "prometheus"
  }

}

kamon {
  environment {
    service = "filodb-local1"
  }
  prometheus.embedded-server {
    hostname = 0.0.0.0
    port = 9095
  }
  modules {
    zipkin-reporter.enabled = false
    prometheus-reporter.enabled = false
    status-page.enabled = false
    otel-trace-reporter.enabled = false
  }

  zipkin {
    url = "https://localhost:9411/api/v2/spans"
    max.requests = 128
    message.max.bytes = 131072
  }

  metric.tick-interval = 60s
  trace {
    identifier-scheme = "single"
    join-remote-parents-with-same-span-id = "true"
    tick-interval = "10s"
    sampler = "random"
    random-sampler.probability = 1.0
  }

  instrumentation.akka.filters {
    "actors.track" {
      includes = [
        "*/user/filo-q*",
        "*/user/node/coordinator/query*",
        "*/user/node/coordinator"
      ]
      excludes = [  ]
    }

    "dispatchers" {
      includes = [ "**" ]
      excludes = [  ]
    }

    "actors.trace" {
      includes = [
        "*/user/filo-q*",
        "*/user/node/coordinator/query*",
        "*/user/node/coordinator"
      ]
    }

    "actors.start-trace" {
      includes = [
        "*/user/filo-q*",
        "*/user/node/coordinator/query*",
        "*/user/node/coordinator"
      ]
      excludes = [  ]
    }
  }
}

akka {
  remote.netty.tcp {
    hostname = "127.0.0.1"
    port = 2552
  }
}

akka-bootstrapper {
  seed-discovery.class = "filodb.akkabootstrapper.ExplicitListClusterSeedDiscovery"
  http-seeds {
    base-url = "http://localhost:8080/"
    retries = 1
  }
  seed-discovery.timeout = 1 minute
  explicit-list.seeds = [
    "akka.tcp://filo-standalone@127.0.0.1:2552"
  ]

}

akka.cluster.downing-provider-class = "org.sisioh.akka.cluster.custom.downing.QuorumLeaderAutoDowning"

custom-downing {
  stable-after = 20s

  quorum-leader-auto-downing {
    role = "worker"
    quorum-size = 1
    down-if-out-of-quorum = true
  }
}
