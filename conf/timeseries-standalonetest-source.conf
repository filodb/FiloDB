    dataset = "prometheus"

    definition {
      partition-columns = ["tags:map"]
      data-columns = ["timestamp:ts", "value:double:detectDrops=true"]
      row-key-columns = [ "timestamp" ]
      downsamplers = [ ]
    }

    options {
      shardKeyColumns = [ "__name__", "_ns" ]
      ignoreShardKeyColumnSuffixes = { "__name__" = ["_bucket", "_count", "_sum"] }
      valueColumn = "value"
      metricColumn = "__name__"
      ignoreTagsOnPartitionKeyHash = [ "le" ]
      copyTags = { }
    }

    num-shards = 4
    min-num-nodes = 2
    sourcefactory = "filodb.kafka.KafkaIngestionStreamFactory"
    sourceconfig {
      # Required FiloDB configurations
      filo-topic-name = "timeseries-dev"
      bootstrap.servers = "localhost:9092"
      group.id = "filo-db-timeseries-ingestion"

      # Values controlling in-memory store chunking, flushing, etc.
      store {
        # Standalone tests need a short flush interval to ensure it finishes running
        flush-interval = 2 minutes
        disk-time-to-live = 2 hours

        demand-paged-chunk-retention-period = 12 hours
        max-chunks-size = 400

        shard-mem-size = 512MB
        ingestion-buffer-mem-size = 200MB
        groups-per-shard = 20
        multi-partition-odp = false
      }
    }